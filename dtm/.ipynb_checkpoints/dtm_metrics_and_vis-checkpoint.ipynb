{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pylab as plt \n",
    "import seaborn as sb \n",
    "import json  \n",
    "import os\n",
    "import errno\n",
    "\n",
    "from time import time \n",
    "import datetime as dt\n",
    "\n",
    "import re\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "#tópicos\n",
    "import gensim\n",
    "import pyLDAvis \n",
    "from gensim.models.ldaseqmodel import LdaSeqModel\n",
    "from gensim.models.wrappers import DtmModel\n",
    "\n",
    "#metricas\n",
    "from gensim.matutils import cossim\n",
    "from itertools import combinations\n",
    "from dit.divergences import jensen_shannon_divergence\n",
    "from dit import ScalarDistribution\n",
    "from tmtoolkit.topicmod.evaluate import metric_cao_juan_2009\n",
    "from tmtoolkit.topicmod.evaluate import metric_arun_2010\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para que no mostrar los warnings\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PostProcesamiento\n",
    "\n",
    "Importar datos procesados, eliminación de palabras poco frecuentes, definicion de slice por año y crear corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sin_fecha_siniestro</th>\n",
       "      <th>sin_relato</th>\n",
       "      <th>sin_relato_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29-08-2013 0:0</td>\n",
       "      <td>DEJE ESTACIONADO LA CAMIONETA POR MAS MENOS PO...</td>\n",
       "      <td>['deje', 'estacionado', 'camioneta', 'menos', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05-09-2013 0:0</td>\n",
       "      <td>DEJE LA CAMIONETA ESTACIONADA EL DIA 04/09 Y H...</td>\n",
       "      <td>['deje', 'camioneta', 'estacionada', 'sali', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05-09-2013 0:0</td>\n",
       "      <td>DEJE EL VEHICULO ESTACIONADO APROXIMADAMENTE A...</td>\n",
       "      <td>['deje', 'estacionado', 'aproximadamente', 'lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05-09-2013 0:0</td>\n",
       "      <td>DEJO VEHICULO PARA HACER CONSULTAS EN LUVAL Y ...</td>\n",
       "      <td>['dejo', 'hacer', 'consultas', 'duval', 'volve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06-09-2013 0:0</td>\n",
       "      <td>EL VIERNES 06 DE SEPTIEMBRE  SALE DEL RESTORAN...</td>\n",
       "      <td>['viernes', 'septiembre', 'sale', 'restoran', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sin_fecha_siniestro                                         sin_relato  \\\n",
       "0      29-08-2013 0:0  DEJE ESTACIONADO LA CAMIONETA POR MAS MENOS PO...   \n",
       "1      05-09-2013 0:0  DEJE LA CAMIONETA ESTACIONADA EL DIA 04/09 Y H...   \n",
       "2      05-09-2013 0:0  DEJE EL VEHICULO ESTACIONADO APROXIMADAMENTE A...   \n",
       "3      05-09-2013 0:0  DEJO VEHICULO PARA HACER CONSULTAS EN LUVAL Y ...   \n",
       "4      06-09-2013 0:0  EL VIERNES 06 DE SEPTIEMBRE  SALE DEL RESTORAN...   \n",
       "\n",
       "                                    sin_relato_clean  \n",
       "0  ['deje', 'estacionado', 'camioneta', 'menos', ...  \n",
       "1  ['deje', 'camioneta', 'estacionada', 'sali', '...  \n",
       "2  ['deje', 'estacionado', 'aproximadamente', 'lu...  \n",
       "3  ['dejo', 'hacer', 'consultas', 'duval', 'volve...  \n",
       "4  ['viernes', 'septiembre', 'sale', 'restoran', ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importar relatos procesados\n",
    "df0 = pd.read_excel('robos_prose_clean_2.0.xlsx')\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_year(fecha):\n",
    "    y=int(re.split('\\W+', fecha)[2])\n",
    "    return y\n",
    "\n",
    "def date_month(fecha):\n",
    "    m = int(re.split('\\W+', fecha)[1])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frecuencia de cada palabra que sobrevivio al preprocesamiento\n",
    "\n",
    "t1=time()\n",
    "\n",
    "corpus = [ast.literal_eval(doc) for doc in df0['sin_relato_clean']]\n",
    "words = [word for doc in corpus for word in doc]\n",
    "freq_per_words = Counter(words)\n",
    "\n",
    "dfreq = pd.DataFrame()\n",
    "dfreq['Palabra'] = freq_per_words .keys()\n",
    "dfreq['Frecuencia'] = freq_per_words .values()\n",
    "\n",
    "t2=time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario:  30284\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño del vocabulario: ', len(dfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deje</td>\n",
       "      <td>12089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>estacionado</td>\n",
       "      <td>27620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>camioneta</td>\n",
       "      <td>6623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>menos</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Palabra  Frecuencia\n",
       "0         deje       12089\n",
       "1  estacionado       27620\n",
       "2    camioneta        6623\n",
       "3        menos         325\n",
       "4        media         441"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30279</th>\n",
       "      <td>furgonque</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30280</th>\n",
       "      <td>rosael</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30281</th>\n",
       "      <td>aprobada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30282</th>\n",
       "      <td>leonora</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30283</th>\n",
       "      <td>sechura</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Palabra  Frecuencia\n",
       "30279  furgonque           1\n",
       "30280     rosael           1\n",
       "30281   aprobada           1\n",
       "30282    leonora           1\n",
       "30283    sechura           1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3381326198577881\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "voc_freq = [*dfreq[dfreq['Frecuencia']>=10]['Frecuencia']]\n",
    "vocabulary = [*dfreq[dfreq['Frecuencia']>=10]['Palabra']]\n",
    "\n",
    "newcorpus = corpus\n",
    "for i, doc in enumerate(corpus):\n",
    "    for j, word in enumerate(doc):\n",
    "        if freq_per_words[word]<10:  \n",
    "            newcorpus[i][j]=''\n",
    "    newcorpus[i] = [elem for elem in newcorpus[i] if elem.strip()]   \n",
    "    \n",
    "t2= time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16156268119812012\n"
     ]
    }
   ],
   "source": [
    "#Frecuencia de cada palabra que sobrevivio al postprocesamiento\n",
    "\n",
    "t1=time()\n",
    "\n",
    "\n",
    "words = [word for doc in newcorpus for word in doc]\n",
    "freq_per_words = Counter(words)\n",
    "\n",
    "dfreq = pd.DataFrame()\n",
    "dfreq['Palabra'] = freq_per_words .keys()\n",
    "dfreq['Frecuencia'] = freq_per_words .values()\n",
    "\n",
    "t2=time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5085</th>\n",
       "      <td>piquetes</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>unos</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4483</th>\n",
       "      <td>acción</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>acorralan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>mochilas</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Palabra  Frecuencia\n",
       "5085   piquetes          10\n",
       "1649       unos          10\n",
       "4483     acción          10\n",
       "3818  acorralan          10\n",
       "2221   mochilas          10"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreq.sort_values('Frecuencia', ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario:  5431\n"
     ]
    }
   ],
   "source": [
    "print('Tamaño del vocabulario: ', len(dfreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['year'] = df0.apply(lambda x: date_year(x['sin_fecha_siniestro']), axis=1)\n",
    "df0['month'] = df0.apply(lambda x: date_month(x['sin_fecha_siniestro']), axis=1)\n",
    "\n",
    "\n",
    "#Nuevo dataframe con los relatos procesados y por año\n",
    "df = pd.DataFrame({'corpus':newcorpus, 'slice':df0['month'].tolist(), 'year':df0['year'].tolist()})\n",
    "df = df[(df['year']>=2011) & (df['year']<=2016)]\n",
    "df.sort_values(['year', 'slice'], ascending=True, inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de relatos por mes\n",
    "time_slices = df.groupby(['year','slice']).count()['index'].tolist()\n",
    "\n",
    "   \n",
    "\n",
    "#Creamos el diccionario a partir de los textos procesados en el formato que necesita el modelo\n",
    "dictionary = gensim.corpora.Dictionary([*df['corpus']])\n",
    "dictionary.save('dictionary.dict')\n",
    "\n",
    "#creamos el corpus para darle al modelo (segun el formato de esta libreria)\n",
    "#El corpus contiene una representacion numerica de los textos, un texto es representada por una lista de tuplas\n",
    "#donde el primer elemento de la tupla es la id de la palabra y el segundo es su frecuencia de aparición en el texto.\n",
    "corpus = [dictionary.doc2bow(text) for text in [*df['corpus']]]\n",
    "gensim.corpora.MmCorpus.serialize('corpora.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar modelos\n",
    "\n",
    "model = []\n",
    "model_name = []\n",
    "K = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for k in K:\n",
    "    model.append(DtmModel.load('dtm_{k}.model'.format(k=k)))\n",
    "    model_name.append('dtm_{k}.model'.format(k=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 $\\text{Entropy}^{1}$\n",
    "\n",
    "\n",
    "$\n",
    " Entropy(K) = \\sum^{M}_{m=1} \\sum^{K}_{k=1} p(z_{m}=k |d=d_{m}) \\left( \\sum^{N_{m}}_{n=1}-p_{t,k,m} \\text{ln } p_{t,k,m} \\right)\n",
    "$, $\\quad \\quad p_{t,k,m} =  \\frac{p(w_{n}=t|z_{n}=k)}{\\sum^{V_{m}}_{n=1}p(w_{n}=t|z_{n}=k)}$\n",
    "\n",
    "http://www.mdpi.com/1099-4300/19/5/173/htm\n",
    "## 1.2 Perplexity\n",
    "\n",
    "$\n",
    "    Perplexity(K) = 2^{Entropy(K)}\n",
    "$\n",
    "\n",
    "\n",
    "Mayor entropia indica mayor es la incertidumbre sobre una variable aleatoria, por tanto menor es mejor, además la enstropia es proporcional a la negative log-likelihood, la cual es positiva, mientras menor es mejor. La perplexity es una función monotona (debido a que es proporcional a la NLL), en particular decreciente, por lo que mayor número de tópicos más pequeña es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_term(k, m, D, dictionary, dict_ptk):\n",
    "    \n",
    "        #Lista de probabilidades de cada palabra del documento m, con repeticion de palabras\n",
    "        N_m = [dictionary[D[m][n][0]] for n in range(len(D[m])) for f in range(D[m][n][1])]\n",
    "\n",
    "        #Lista de probabilidades de cada palabra del documento m, pero sin repetir palabras\n",
    "        V_m = [dictionary[D[m][n][0]] for n in range(len(D[m]))]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Suma de las probabilidades de cada palabra del documento m, pero sin repetir palabras, p[w_n=t|z_n=k] = dict_ptk[k][w]\n",
    "        den = sum([dict_ptk[k][w] for w in V_m])\n",
    "    \n",
    "        return sum([(-dict_ptk[k][w]/den)*np.log(dict_ptk[k][w]/den) for w in N_m])\n",
    "\n",
    "\n",
    "def entropy(time, model):\n",
    "    ntopic = model.num_topics\n",
    "    voc = []\n",
    "    M = time_slices[time]#número de docs en el slice correspondiente\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "    D = corpus[time_slices[time-1]: time_slices[time]+time_slices[time-1]] #corpus del slice escogido\n",
    "\n",
    "    \n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(ntopic):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(ntopic)]\n",
    "    \n",
    "    \n",
    "     \n",
    "        \n",
    "    doc_topics = model.dtm_vis(time=time, corpus=corpus)[0]\n",
    "    \n",
    "    entropy = sum([doc_topics[m, k]*second_term(k, m, D, dictionary, dict_ptk) for m in range(M) for k in range(ntopic)])\n",
    "      \n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular la entropía de todos los modelos\n",
    "e = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        e[k, t] = entropy(t, model[k])\n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guardar los valores de  entropia para cada modelo en un js\n",
    "entropy = [int(e[i].sum()) for i in range(len(e))]\n",
    "filename = \"dtm_html/entropy_data.js\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var entropy_data = {s}\".format(s = entropy))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Average cosine similarity between topic distribution\n",
    "\n",
    "Cao et al. (2009), estima la similitud coseno promedio entre las distribuciones de los tópicos $\\overrightarrow{\\phi_{i}}$, $\\overrightarrow{\\phi_{j}}$ ($ i \\neq j$) y escoge el valor de \\textit{K}\n",
    "que minimiza esta cantidad.\\\\\n",
    "\n",
    "La similitud coseno calcula la norma L2 del producto punto de dos vectores. Esto es, si x e y son vectores, su similitud coseno $k$ es definida como:\n",
    "\n",
    "$$k(x, y) = \\frac{xy^{T}}{||x||||y||}$$<br>\n",
    "\n",
    "Luego la similitud coseno promedio entre las distribuciones de los tópicos queda definida como sigue\n",
    "$$avg\\_cosine\\_distance = \\frac{\\sum^{K}_{i=0}\\sum^{K}_{j=i+1}k(\\phi_{i},\\phi_{j})}{K \\times(K-1)/2 }$$\n",
    "\n",
    "https://stackoverflow.com/questions/942543/operation-on-every-pair-of-element-in-a-list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar la combinatoria de la métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cosine_distance(time, model): \n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)]\n",
    "    \n",
    "    sum_cosine_distance = 0\n",
    "    \n",
    "    #Calculo la similitud para todas las combinaciones de tópicos\n",
    "    pairs = [*combinations(range(K-1), 2)]\n",
    "    for p in pairs :\n",
    "            sum_cosine_distance+= cossim(dict_ptk[p[0]], dict_ptk[p[1]])\n",
    "            \n",
    "    avg_cosine_distance = sum_cosine_distance/len(pairs)\n",
    "    return avg_cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04284787178039551\n"
     ]
    }
   ],
   "source": [
    "#prueba \n",
    "t1 = time()\n",
    "avg_cosine_distance(1, model[1])\n",
    "t2=time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3347764015197754\n",
      "2.3118460178375244\n",
      "4.08141565322876\n",
      "5.890650272369385\n",
      "8.021163702011108\n",
      "11.037850618362427\n",
      "13.540852308273315\n",
      "19.081740140914917\n",
      "20.48908257484436\n"
     ]
    }
   ],
   "source": [
    "#Calcular la avg_cosine_distance de todos los modelos en todos sus períodos\n",
    "avg_cossim = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_cossim[k, t] = avg_cosine_distance(time=t, model=model[k]) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_cosine_distance para cada modelo en un js\n",
    "avgcossim = [round(avg_cossim[i].mean(), 2) for i in range(len(avg_cossim))]\n",
    "filename = \"dtm_html/avgcossim_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgcossim_data = {s}\".format(s = avgcossim))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Average Jensen-Shannon distance between topic distribution\n",
    "\n",
    "\n",
    "Deveaud et al. (2014) [4] máximiza la distancia \\textit{Jensen-Shannon} promedio entre las distribuciones de tópicos  $\\overrightarrow{\\phi_{i}}$, $\\overrightarrow{\\phi_{j}}$ ($ i \\neq j$), muy similiar a Cao et al. (2009).\\\\\n",
    "\n",
    "$$avg\\_js\\_distance = \\frac{\\sum^{K}_{i=0}\\sum^{K}_{j=i+1}D(\\phi_{i},\\phi_{j})}{K \\times(K-1)/2 }$$\n",
    "\n",
    "\n",
    "Donde $D$ es la divergencia de Jensen-Shannon, la cual es una medida de divergencia basada en principios que siempre es finita para las variables aleatorias finitas. Cuantifica cuán \"distinguibles\" son dos o más distribuciones entre sí. En su forma básica es:\n",
    "\n",
    "$$D(X||Y) = \\frac{H(X+Y)}{2}+\\frac{H(X)+H(Y)}{2}$$\n",
    "\n",
    "Donde $H$ es la entropia, la entropía de una variable aleatoria $X$ con posibles valores ${x_{1}, .., x_{n}}$, se define de la siguiente forma:\n",
    "\n",
    "$$H(X) = -\\sum^{n}_{i=1}P(x_{i})log(P(x_{i}) = \\sum^{V}_{w=1} \\phi^{i}_{w}log(\\phi^{i}_{w}), \\forall i \\in {1, ..., K}$$\n",
    "\n",
    "\n",
    "Considering it is a non-symmetric measure, we use the Jensen-Shannon divergence,\n",
    "which is a symmetrised version of the KL divergence, to avoid obvious problems when\n",
    "computing divergences between all pairs of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_js_distance(time, model):\n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    sum_js_distance = 0\n",
    "        \n",
    "    #Calculo la similitud para todas las combinaciones de tópicos\n",
    "    pairs = [*combinations(range(K-1), 2)]\n",
    "    for p in pairs :\n",
    "            X = ScalarDistribution([*dict_ptk[p[0]].keys()], [*dict_ptk[p[0]].values()])\n",
    "            Y = ScalarDistribution([*dict_ptk[p[1]].keys()], [*dict_ptk[p[1]].values()])\n",
    "            \n",
    "            sum_js_distance+= jensen_shannon_divergence([X, Y])\n",
    "            \n",
    "    avg_js_distance = sum_js_distance/len(pairs)  \n",
    "    return avg_js_distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.262650966644287\n"
     ]
    }
   ],
   "source": [
    "#prueba\n",
    "t1 = time()\n",
    "avg_js_distance(time=1, model=model[1]) \n",
    "t2 = time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.86623668670654\n",
      "249.50126361846924\n",
      "473.1665823459625\n",
      "779.6344659328461\n",
      "1494.8150265216827\n",
      "3610.90580368042\n",
      "4598.188099384308\n",
      "3278.9420113563538\n",
      "3702.1329233646393\n"
     ]
    }
   ],
   "source": [
    "#Calcular la avg_js_distance de todos los modelos en todos sus períodos\n",
    "avg_js = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_js[k, t] = avg_js_distance(time=t, model=model[k]) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_js_distance para cada modelo en un js\n",
    "avgjs= [round(avg_js[i].mean(),2) for i in range(len(avg_js))]\n",
    "filename = \"dtm_html/avgjs_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgjs_data = {s}\".format(s = avgjs))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Arun et al. (2010)\n",
    "\n",
    "\n",
    "Arun et al. (2010) minimiza la \\textit{Symetric Kullback-Liebler divergence (Jensen-Shannon distance)} entre los valores singulares de la representación matricial de las probabilidades de cada tópico ($\\beta$) y la distribución de tópicos ($\\theta$) dentros del corpus.\n",
    "\n",
    "\n",
    "$$Arun\\_Measure(M1,M2) = KL(CM1||CM2) + KL(CM2||CM1) $$\n",
    "\n",
    "Donde, <br>\n",
    "$CM1$ es la distribución de los valores singulares de la topic-word matriz M1, <br>\n",
    "$CM2$ es la distribución obtenida normalizado por el vector $L∗M2$ (donde $L$ es <br> un vector de dimensión $D$  que indica el largo de cada documento en el corpus y $M2$ es la matriz topic-document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_arun(time, model): \n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)] # topic distribution\n",
    "    #Pasar la distribucion de los tópicos al formato de la liberia que calcula la metrica\n",
    "    d = pd.DataFrame(dict_ptk) # topic distribution\n",
    "    topic_word_distrib = np.array(d)\n",
    "    \n",
    "    doc_topics = model.dtm_vis(time=time, corpus=corpus)[0] #mezcla de tópicos de los documentos\n",
    "    slices = time\n",
    "    # mezcla de tópicos de los documentos para el slice de evaluacion\n",
    "    doc_topic_distrib = np.array(doc_topics[sum(time_slices[0:time]):sum(time_slices[0:time+1])]) \n",
    "    \n",
    "    #largo de cada documento en terminos de palabras\n",
    "    c = corpus[sum(time_slices[0:time]):sum(time_slices[0:time+1])]\n",
    "    doc_len=[]\n",
    "    for i in range(len(c)):\n",
    "        s = 0\n",
    "        for j in range(len(c[i])):\n",
    "            s += c[i][j][1]\n",
    "        doc_len.append(s)\n",
    "        \n",
    "    doc_lengths = np.array([doc_len])\n",
    "    \n",
    "    return metric_arun_2010(topic_word_distrib, doc_topic_distrib, doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46573567390441895\n"
     ]
    }
   ],
   "source": [
    "#prueba\n",
    "t1 = time()\n",
    "metric_arun(1, model[2])\n",
    "t2 = time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular la avg_js_distance de todos los modelos en todos sus períodos\n",
    "avg_arun = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_arun[k, t] = metric_arun(time=t, model=model[k]) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_js_distance para cada modelo en un js\n",
    "avgarun= [round(avg_arun[i].mean(),2) for i in range(len(avg_arun))]\n",
    "filename = \"dtm_html/avgarun_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgarun_data = {s}\".format(s = avgarun))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Griffiths et al. (2010)\n",
    "\n",
    "\n",
    "Griffiths y Syteyvers  (2004) dicen: \"Para un estadístico Bayesiano enfrentado a una elección entre un conjunto de modelos estadísticos, la respuesta natural es calcular el probabilidad posterior de ese conjunto de modelos dado el observado datos.\" El componente clave de esta probabilidad posterior será el probabilidad de los datos dados el modelo, integrando sobre todos\n",
    "los parámetros en el modelo.<br>\n",
    "\n",
    "En este caso, la data son las palabas en el corpus, $w$, y el modelo es especificado por el número de tópics, $T$, entonces entonces deseamos calcular la probabilidad de palabras para los temas $z$.<br>\n",
    "\n",
    "Como siempre la likelihood es difícil de calcular, entonces esta se puede aproximar $P(w|T)$, tomando la media armónica de un conjunto de valores de $P(z|w, T)$ cuando $z$ es generado de la posterior $P(z|w, T)$. Entonces el algoritmo Gibb sampling provee tales ejemplos, y el valor de $P(w|z, T)$ pude ser computado. Luego elijen el $K$ que maximiza la media ármonica de la log-likelihoods muestreada.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.evaluate import metric_griffiths_2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "logliks = model[0].lhood_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_griffiths_2004?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_griffiths_2004(logliks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta métrica sirve para comparar entre modelos con el mismo número de tópicos, a más tópicos es estrictamente decreciente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Coherence score - U mass\n",
    "\n",
    "Mimno et al (2011) propone una métrica para evaluación de la calidad de un tópico. Dado un tópico $z$ y sus top $T$ palabras $V^{(z)}=(v^{(z)}_{1}, ..., v^{(z)}_{T})$ ordenadas por $P(w|z)$, la $coherence score$ es definido como:\n",
    "\n",
    "$$C(z;V^{(z)}) = \\sum^{T}_{t=2}\\sum^{t}_{l=1}log \\frac {D(v^{(z)}_{m}, v^{(z)}_{l})+1}{D(v^{(z)}_{l})}$$\n",
    "\n",
    "Donde $D(v)$ es la frecuencia de la palabra $v$ a nivel documento, $D(v,v^{'})$ es el número de documentos donde la palabra $v$ y $v'$ co-ocurren. El coherence score se basa en la idea que las palabras dentro de un único concepto tenderán a co-ocurrir dentro del mismo documento. Esto esta empíricamente demostrado que el coherence score es altamente correlacionado con la coherencia de tópicos del juicio humano. Debe enfatizarse que el puntaje de coherencia solo es apropiado para medir palabras frecuentes en un tema. Porque la frecuencia de palabras raras es menos confiable.<br>\n",
    "\n",
    "\n",
    "\n",
    "Luego se escoge $k$ que máximiza la coherencia promedio:\n",
    "\n",
    "$$avg\\_coherence\\_score = \\frac{1}{K}\\sum^{K}_{k=1} C(z_{k};V^{(z_{k})})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_score(time, model, topn):\n",
    "    topics_wrapper = model.dtm_coherence(time=time)\n",
    "    cm_wrapper = CoherenceModel(topics=topics_wrapper, corpus=corpus, dictionary=dictionary, coherence='u_mass', topn=topn)\n",
    "    \n",
    "    coherence = cm_wrapper.get_coherence()\n",
    "    \n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3102695941925049\n"
     ]
    }
   ],
   "source": [
    "#prueba\n",
    "t1=time()\n",
    "coherence_score(time=1, model=model[0], topn=20)\n",
    "t2=time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.3764705657959\n",
      "24.830121994018555\n",
      "28.218249797821045\n",
      "28.622239112854004\n",
      "35.29431939125061\n",
      "33.817896604537964\n",
      "34.77373766899109\n",
      "33.54385709762573\n",
      "34.262282609939575\n"
     ]
    }
   ],
   "source": [
    "#Calcular la avg_js_distance de todos los modelos en todos sus períodos\n",
    "avg_coherence_score = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_coherence_score[k, t] =  coherence_score(time=t, model=model[k], topn=20) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_js_distance para cada modelo en un js\n",
    "avgcoherence= [round(avg_coherence_score[i].mean(),2) for i in range(len(avg_coherence_score))]\n",
    "filename = \"dtm_html/avgcoherence_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgcoherence_data = {s}\".format(s = avgcoherence))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LDAVis\n",
    "\n",
    "Esta parte consiste en guardar la data necesaria para realizar las visualizaciones con la librería LDAVis en un diccionario, luego se guarda ese diccionario en una variable en un archivo js que luego será leído por el script a cargo de las visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.39201378822327\n",
      "58.38373398780823\n",
      "60.51923060417175\n",
      "64.82786440849304\n",
      "64.37093257904053\n",
      "68.16515827178955\n",
      "76.49615502357483\n",
      "72.90783071517944\n",
      "80.07492733001709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "slices = time_slices\n",
    "\n",
    "for k in range(len(model)):\n",
    "    t1 = time()\n",
    "    dtm = []\n",
    "    for i in range(len(slices)):\n",
    "        doc_topic, topic_term, doc_lengths, term_frequency, vocab = model[k].dtm_vis(time=i, corpus=corpus)\n",
    "        vis_wrapper = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency, sort_topics=False, R=30)\n",
    "        data = pyLDAvis.save_json(vis_wrapper, 'dtm_html/data.js')\n",
    "        with open('dtm_html/data.js') as f:\n",
    "            dtm.append(f.read())\n",
    "\n",
    "    dtm_data = {}\n",
    "    for i in range(len(dtm)):\n",
    "        dtm_data[date[i]] = ast.literal_eval(dtm[i])\n",
    "\n",
    "\n",
    "\n",
    "    dtm_data_json = json.dumps(dtm_data) #parsing\n",
    "\n",
    "    filename = \"dtm_html/dtm.js\"\n",
    "    if not os.path.exists(filename):\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"var dtm_data = {}; \\n\")\n",
    "                f.close()  \n",
    "                \n",
    "                \n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"dtm_data[{ntopics}] = \".format(ntopics=model_name[k].split('.')[0].split('_')[1]) + repr(ast.literal_eval(dtm_data_json)) + \"; \\n\")\n",
    "        f.close()  \n",
    "    t2 = time()\n",
    "    print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Serie de tiempo \n",
    "\n",
    "Esta parte consiste en guardar en un js un diccionario que contiene los tamaños de los tópicos en el tiempo de cada modelo, para luego ser leído por un script que visualiza las series de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>slice</th>\n",
       "      <th>counts</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>475</td>\n",
       "      <td>Jan</td>\n",
       "      <td>Jan 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>302</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>660</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>544</td>\n",
       "      <td>Apr</td>\n",
       "      <td>Apr 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>634</td>\n",
       "      <td>May</td>\n",
       "      <td>May 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  slice  counts month      date\n",
       "0  2011      1     475   Jan  Jan 2011\n",
       "1  2011      2     302   Feb  Feb 2011\n",
       "2  2011      3     660   Mar  Mar 2011\n",
       "3  2011      4     544   Apr  Apr 2011\n",
       "4  2011      5     634   May  May 2011"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "dft = df.groupby(['year', 'slice']).size().reset_index(name='counts')\n",
    "dft['month'] = month*6\n",
    "dft['date'] = dft.apply(lambda x: x['month']+' '+str(x['year']), axis=1)\n",
    "date = dft['date'].values.tolist()\n",
    "\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39002013206481934\n",
      "0.40934205055236816\n",
      "0.4426252841949463\n",
      "0.4508399963378906\n",
      "0.4085359573364258\n",
      "0.41791415214538574\n",
      "0.42386627197265625\n",
      "0.44089555740356445\n",
      "0.4330294132232666\n"
     ]
    }
   ],
   "source": [
    "slices = time_slices\n",
    "series = []\n",
    "\n",
    "for j in range(len(model)):\n",
    "    t1 = time()\n",
    "    \n",
    "    doc_topic = model[j].dtm_vis(time=0, corpus=corpus)[0]\n",
    "    doc_topic_dist = doc_topic\n",
    "    doc_topic = [np.argmax(doc_topic_dist[i]) for i in range(len(doc_topic_dist))]\n",
    "\n",
    "    t = time_slices\n",
    "    b = np.array(doc_topic)\n",
    "    u = np.unique(b)\n",
    "\n",
    "    t = time_slices\n",
    "    t = [0]+t\n",
    "    c = [len(b[sum(t[0:i]): sum(t[0:i])+t[i+1]] [b[sum(t[0:i]): sum(t[0:i])+t[i+1]] == v]) for v in u for i in range(len(t)-1)]\n",
    "\n",
    "    \n",
    "    dft = df.groupby(['year', 'slice']).size().reset_index(name='counts')\n",
    "    date = [dt.datetime( year=dft['year'][i] , month=dft['slice'][i], day=1).ctime() for i in range(len(dft))]\n",
    "\n",
    "    d = []\n",
    "    for i in range(len(slices)):\n",
    "        d1 = {}\n",
    "        d1[str(0)] = int(dft['counts'][i])\n",
    "        for k in u:\n",
    "            d1[str(k+1)] = c[len(slices)*k+i]\n",
    "        d1['date'] = date[i]\n",
    "\n",
    "        d.append(d1)\n",
    "    \n",
    "    data_series_json = json.dumps(d) #parsing\n",
    "  \n",
    "    filename = \"dtm_html/series_data.js\"\n",
    "    if not os.path.exists(filename):\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"var series_data  = {}; \\n\")\n",
    "                f.close()  \n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"series_data[{ntopics}] = \".format(ntopics=model_name[j].split('.')[0].split('_')[1]) + repr(ast.literal_eval(data_series_json)) + \"; \\n\")\n",
    "        f.close()  \n",
    "    t2 = time()\n",
    "    print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script es para pasarle los datos que genero el script anterior en un formato que la librería que visualiza la serie de tiempo entiende mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029916763305664062\n",
      "0.0009975433349609375\n",
      "0.0009975433349609375\n",
      "0.0009968280792236328\n",
      "0.0009975433349609375\n",
      "0.000997304916381836\n",
      "0.000997304916381836\n",
      "0.0019948482513427734\n",
      "0.0009970664978027344\n"
     ]
    }
   ],
   "source": [
    "filename = \"dtm_html/graph_data.js\"\n",
    "if not os.path.exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"var graph_data  = {}; \\n\")\n",
    "            f.close()  \n",
    "\n",
    "color = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#d2f53c', '#fabebe']\n",
    "for k in K:\n",
    "    t1 = time()\n",
    "    data = []\n",
    "    d = {\"id\": \"g\",\n",
    "         \"valueAxis\": \" \",\n",
    "         \"lineColor\": \"#808080\",\n",
    "         \"bullet\": \"round\",\n",
    "         \"bulletBorderThickness\": 1,\n",
    "         \"hideBulletsCount\": 30,\n",
    "         \"title\": \"Total\",\n",
    "         \"valueField\": \"0\",\n",
    "         \"fillAlphas\": 0}\n",
    "\n",
    "    data.append(d)\n",
    "    for j in range(k):\n",
    "\n",
    "        d = {\"id\": \"g{n}\".format(n=j+1),\n",
    "            \"valueAxis\": \" \",\n",
    "            \"lineColor\": color[j],\n",
    "            \"bullet\": \"round\",\n",
    "            \"bulletBorderThickness\": 1,\n",
    "            \"hideBulletsCount\": 30,\n",
    "            \"title\": \"Topic{n}\".format(n=j+1),\n",
    "            \"valueField\": \"{n}\".format(n=j+1),\n",
    "            \"fillAlphas\": 0}\n",
    "\n",
    "        data.append(d)\n",
    "        \n",
    "    graph_data_json = json.dumps(data) #parsing    \n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"graph_data[{n}] = \".format(n=k) + repr(ast.literal_eval(graph_data_json)) + \"; \\n\")\n",
    "        f.close()  \n",
    "    t2 = time()\n",
    "    print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>475</td>\n",
       "      <td>47</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>172</td>\n",
       "      <td>Sat Jan  1 00:00:00 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>302</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>114</td>\n",
       "      <td>Tue Feb  1 00:00:00 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>65</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>64</td>\n",
       "      <td>73</td>\n",
       "      <td>264</td>\n",
       "      <td>Tue Mar  1 00:00:00 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>544</td>\n",
       "      <td>50</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>201</td>\n",
       "      <td>Fri Apr  1 00:00:00 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>634</td>\n",
       "      <td>53</td>\n",
       "      <td>52</td>\n",
       "      <td>56</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>53</td>\n",
       "      <td>78</td>\n",
       "      <td>219</td>\n",
       "      <td>Sun May  1 00:00:00 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1  10   2   3   4  5   6   7   8    9                      date\n",
       "0  475  47  30  41  32  32  0  19  48  54  172  Sat Jan  1 00:00:00 2011\n",
       "1  302  21  26  22   9  25  0  15  36  34  114  Tue Feb  1 00:00:00 2011\n",
       "2  660  65  52  53  31  29  0  29  64  73  264  Tue Mar  1 00:00:00 2011\n",
       "3  544  50  45  46  31  37  0  24  45  65  201  Fri Apr  1 00:00:00 2011\n",
       "4  634  53  52  56  32  59  0  32  53  78  219  Sun May  1 00:00:00 2011"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ejemplo de datos que usa la libería que visualiza la serie de tiempo del tamaño de los tópicos\n",
    "pd.read_pickle('dtm_serie_10.pkl').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendientes\n",
    "\n",
    "\n",
    "#### Aspectos técnicos \n",
    "\n",
    "1. Ordenar el código de las visualizaciones, como también los ficheros que generan. \n",
    "2. Exportar el js sin la necesidad de un entorno virtual como IPython para elmodulo pyLDAvis.\n",
    "3. Ver el parsing con las tildes: Es necesario pasar a JSON, o directamente usar diccionario de Python\n",
    "4. Organizar código de procesamiento y postprocesamiento\n",
    "5. Implementación .py\n",
    "6. GITHUB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Correr con diferentes número de tópicos para este mismo slice\n",
    "4. Ver el parsing con las tildes: Es necesario pasar a JSON, o directamente usar diccionario de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Versión alternativa similitud coseno, pero más lenta\n",
    "\n",
    "\n",
    "def metric_cao_juan(time, model):\n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)] # topic distribution\n",
    "    #Pasar la distribucion de los tópicos al formato de la liberia que calcula la metrica\n",
    "    d = pd.DataFrame(dict_ptk) # topic distribution\n",
    "    topic_word_distrib = [[*d.iloc[0,:]],[*d.iloc[1,:]]]\n",
    "\n",
    "\n",
    "    return metric_cao_juan_2009(topic_word_distrib)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
