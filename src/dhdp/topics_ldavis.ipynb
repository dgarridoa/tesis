{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "from dotenv import load_dotenv\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge pyldavis=2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = os.getenv(\"CORPUS\") \n",
    "model_path = os.getenv(\"MODEL_PATH\")\n",
    "dict_files = sorted([file for file in os.listdir(corpus) if \".dict\" in file])\n",
    "models_dir = sorted(os.listdir(model_path))\n",
    "\n",
    "epochs = range(0, len(models_dir))\n",
    "data = {}\n",
    "for epoch in epochs:\n",
    "     # load dictionary {word->id}\n",
    "    dict_path = f'{corpus}/{dict_files[epoch]}'\n",
    "    token2id = Dictionary.load(dict_path).token2id\n",
    "    vocabulary = list(token2id.keys())\n",
    "    \n",
    "    # load topics distributions\n",
    "    topics_path = f'{model_path}/{models_dir[epoch]}/mode-topics.dat'\n",
    "    with open(topics_path, \"r\") as f:\n",
    "        topics = np.array([[int(word) for word in line.strip().split()] for line in f])\n",
    "    topics_dists = (topics.T/topics.sum(axis=1)).T\n",
    "    \n",
    "    # load word assignments and get mixture weigths\n",
    "    word_assignments_path = f\"{model_path}/{models_dir[epoch]}/mode-word-assignments.dat\"\n",
    "    word_assignments = []\n",
    "    with open(word_assignments_path, \"r\") as f:\n",
    "        lines = f.readlines()[1:]\n",
    "    # (doc_id, word_id, topic_id, x)\n",
    "    for line in lines:\n",
    "        line = line.strip().split() \n",
    "        word_assignment = {\"doc_id\": int(line[0]) ,\"word_id\": int(line[1]), \"topic_id\": int(line[2])}\n",
    "        word_assignments.append(word_assignment)\n",
    "    \n",
    "    # for each document get the number of words that each topic has\n",
    "    word_assignments = pd.DataFrame(word_assignments, columns = [\"doc_id\", \"word_id\", \"topic_id\"])\n",
    "    term_frequency = word_assignments.groupby(\"word_id\").size().values\n",
    "    word_assignments = word_assignments.pivot_table(index = \"doc_id\", columns = \"topic_id\", aggfunc = \"count\")\n",
    "    word_assignments.fillna(0, inplace = True)\n",
    "    \n",
    "    # for each document get the number of words\n",
    "    doc_length = word_assignments.sum(axis = 1) \n",
    "    \n",
    "    # for each document get the topic proportions that draw the document (\\pi_d)\n",
    "    doc_topic_dists = word_assignments.divide(doc_length, axis = 0).values\n",
    "\n",
    "    #save data in a dict\n",
    "    data[epoch] = {\"topic_term_dists\": topics_dists, \"doc_topic_dists\": doc_topic_dists, \n",
    "                   \"term_frequency\": term_frequency, \"doc_lengths\": doc_length.values, \"vocab\": vocabulary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get data ldavis_data and save\n",
    "for epoch in epochs:\n",
    "    hdp_ldavis_data = pyLDAvis.prepare(mds = \"tsne\", **data[epoch]).to_dict()\n",
    "    hdp_ldavis_data[\"tinfo\"][\"logprob\"] = [-1e16 if elem==float(\"-inf\") else elem for elem in hdp_ldavis_data[\"tinfo\"][\"logprob\"]]\n",
    "    hdp_ldavis_data[\"tinfo\"][\"loglift\"] = [-1e16 if elem==float(\"-inf\") else elem for elem in hdp_ldavis_data[\"tinfo\"][\"loglift\"]]\n",
    "    with open(f\"../../vis/data/{epoch+1}.json\", \"w\") as f:\n",
    "        json.dump(hdp_ldavis_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldavis_data for last slice\n",
    "hdp_ldavis_data = pyLDAvis.prepare(mds=\"tsne\", **data[slice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display ldavis\n",
    "pyLDAvis.display(hdp_ldavis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
