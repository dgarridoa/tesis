{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from time import time\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.bleicorpus import BleiCorpus\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "from wmd import wmd\n",
    "from dotenv import load_dotenv\n",
    "from scipy.sparse import csc_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = f'{os.getenv(\"CORPUS\")}{os.getenv(\"SLICE_TYPE\")}'\n",
    "corpus_files = sorted([file for file in os.listdir(corpus_dir) if bool(re.match(\"corpus_\\d*.mm$\", file))])\n",
    "dict_files = sorted([file for file in os.listdir(corpus_dir) if \".dict\" in file])\n",
    "models_path = f'{os.getenv(\"RESULTS\")}hdp/{os.getenv(\"SLICE_TYPE\")}'\n",
    "models_dir = sorted(os.listdir(models_path))\n",
    "\n",
    "slices = range(1, len(models_dir)+1)\n",
    "data = {}\n",
    "for slice in slices:\n",
    "    # load dictionary {word->id}\n",
    "    dict_path = f'{corpus_dir}/{dict_files[slice-1]}'\n",
    "    token2id = Dictionary.load(dict_path).token2id\n",
    "\n",
    "    # get term probability\n",
    "    corpus_path = f'{corpus_dir}/{corpus_files[slice-1]}'\n",
    "    corpus = BleiCorpus(corpus_path)\n",
    "    term_frequency = np.zeros(len(token2id))\n",
    "    for doc in corpus:\n",
    "        for (id_word, freq) in doc:\n",
    "            term_frequency[id_word]+=freq\n",
    "    term_probability = term_frequency/term_frequency.sum()\n",
    "    \n",
    "    # load topics distributions\n",
    "    topics_path = f'{models_path}/{models_dir[slice-1]}/mode-topics.dat'\n",
    "    with open(topics_path, \"r\") as f:\n",
    "        topics = np.array([[int(word) for word in line.strip().split()] for line in f])\n",
    "    topics_dists = (topics.T/topics.sum(axis=1)).T\n",
    "\n",
    "    # save data in a dict\n",
    "    data[slice] = {\"token2id\": token2id, \"topics_dists\": topics_dists, \"term_probability\": term_probability}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_facebook_vectors(os.getenv(\"EMBEDDINGS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_matrix(data, size):\n",
    "    sparse_matrix = csc_matrix(size, dtype=np.int8)\n",
    "    for (i,j) in data:\n",
    "        sparse_matrix[i-1,j-1]=1\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[(1,4), (2,2), (3,2), (4,1), (5,5), (6,3), (7,2)],\n",
    "[(1,4), (2,2), (2,3), (3,5), (4,1), (5,6)],\n",
    "[(1,4), (2,5), (3,2), (4,5), (5,1), (6,6)],\n",
    "[(1,2), (2,4), (4,5), (5,4), (5,7), (6,3)],\n",
    "[(1,3), (2,2), (3,7), (4,5), (5,6), (7,1)]]\n",
    "ground_truth = []\n",
    "for slice in slices[:-1]:\n",
    "    K1 = len(data[slice][\"topics_dists\"])\n",
    "    K2 = len(data[slice+1][\"topics_dists\"])\n",
    "    sparse_matrix = get_sparse_matrix(labels[slice-1], (K1, K2))\n",
    "    ground_truth.append(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_graph(data, q, alpha):\n",
    "    similarity_graph = []\n",
    "    for slice in slices[:-1]:\n",
    "        token2id1 = data[slice][\"token2id\"]\n",
    "        token2id2 = data[slice+1][\"token2id\"]\n",
    "        topics_dists1 = data[slice][\"topics_dists\"] \n",
    "        topics_dists2 = data[slice+1][\"topics_dists\"]\n",
    "        termp1 = data[slice][\"term_probability\"]\n",
    "        termp2 = data[slice+1][\"term_probability\"]\n",
    "        K1 = len(topics_dists1)\n",
    "        K2 = len(topics_dists2)\n",
    "        similarity_matrix = np.zeros((K1, K2))\n",
    "        for i in range(K1):\n",
    "            topic_i = topics_dists1[i]\n",
    "            for j in range(K2):\n",
    "                topic_j = topics_dists2[j]\n",
    "                distance = wmd(embeddings, token2id1, token2id2, topic_i, topic_j, \n",
    "                               termp1, termp2, q=q, alpha=alpha)\n",
    "                similarity = 1/(1+distance)\n",
    "                similarity_matrix[i,j] = similarity\n",
    "        similarity_graph.append(similarity_matrix)\n",
    "    return similarity_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_graph(graph, zeta):\n",
    "    edges_dist = np.concatenate([matrix.flatten() for matrix in graph])\n",
    "    threshold = np.quantile(edges_dist, zeta)\n",
    "    pruned_graph = [csc_matrix((matrix>threshold).astype(int)) for matrix in graph]\n",
    "    return pruned_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(ground_truth, prediction):\n",
    "    N = len(ground_truth)\n",
    "    y_true = np.concatenate([ground_truth[i].toarray().flatten() for i in range(N)])\n",
    "    y_pred = np.concatenate([prediction[i].toarray().flatten() for i in range(N)])\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f_score = f1_score(y_true, y_pred)\n",
    "    return recall, precision, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_graph = build_similarity_graph(data, q=0.8, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0.08635097493036212, 0.15897435897435896),\n",
       " (1.0, 0.09117647058823529, 0.16711590296495957),\n",
       " (1.0, 0.09657320872274143, 0.17613636363636365),\n",
       " (1.0, 0.10264900662251655, 0.18618618618618618),\n",
       " (1.0, 0.10954063604240283, 0.19745222929936307),\n",
       " (1.0, 0.11742424242424243, 0.21016949152542375),\n",
       " (1.0, 0.12601626016260162, 0.22382671480144403),\n",
       " (1.0, 0.13656387665198239, 0.24031007751937986),\n",
       " (0.967741935483871, 0.14423076923076922, 0.2510460251046025),\n",
       " (0.967741935483871, 0.15873015873015872, 0.2727272727272727),\n",
       " (0.967741935483871, 0.17647058823529413, 0.2985074626865672),\n",
       " (0.967741935483871, 0.1986754966887417, 0.3296703296703297),\n",
       " (0.967741935483871, 0.22727272727272727, 0.36809815950920244),\n",
       " (0.967741935483871, 0.2631578947368421, 0.4137931034482759),\n",
       " (0.967741935483871, 0.3157894736842105, 0.47619047619047616),\n",
       " (0.967741935483871, 0.39473684210526316, 0.5607476635514018),\n",
       " (0.9032258064516129, 0.49122807017543857, 0.6363636363636364),\n",
       " (0.8064516129032258, 0.6578947368421053, 0.7246376811594202),\n",
       " (0.6129032258064516, 1.0, 0.76)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metrics(ground_truth, pruning_graph(similarity_graph, zeta)) for zeta in np.arange(0.05, 1, 0.05).round(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:0.2, alpha: 0.2\n"
     ]
    }
   ],
   "source": [
    "grid = {\"q\": [0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99, 1.0], \"alpha\":np.linspace(0.2,1,5).round(1), \"zeta\": np.arange(0.05, 1, 0.05).round(2)}\n",
    "graphs = []\n",
    "performance = []\n",
    "for q in grid[\"q\"]:\n",
    "    for alpha in grid[\"alpha\"]:\n",
    "        print(f\"q:{q}, alpha: {alpha}\")\n",
    "        ti = time()\n",
    "        similarity_graph = build_similarity_graph(data, q, alpha)\n",
    "        tf = time()\n",
    "        delta_time = round(tf-ti)\n",
    "        for zeta in grid[\"zeta\"]:\n",
    "            prediction = pruning_graph(similarity_graph, zeta)\n",
    "            recall, precision, f_score = metrics(ground_truth, prediction)\n",
    "            performance.append([q, alpha, delta_time, zeta, recall, precision, f_score])\n",
    "        graphs.append({\"q\":q, \"alpha\":alpha, \"time[s]\":delta_time ,\"graph\":similarity_graph})\n",
    "df_performance = pd.DataFrame(results, columns = [\"q\", \"alpha\", \"zeta\", \"recall\", \"precision\", \"f-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "ground_truth_path = f'{os.getenv(\"RESULTS\")}graph/ground_truth.pkl'\n",
    "with open(ground_truth_path, \"wb\") as f:\n",
    "    pkl.dump(ground_truth, f, pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "graph_path = f'{os.getenv(\"RESULTS\")}graph/graphs.pkl'\n",
    "with open(graph_path, \"wb\") as f:\n",
    "    pkl.dump(graphs, f, pkl.HIGHEST_PROTOCOL)\n",
    "    \n",
    "df_path = f'{os.getenv(\"RESULTS\")}graph/performance.csv'\n",
    "df_performance.to_csv(    , sep=\"|\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns = [\"q\", \"alpha\", \"zeta\", \"recall\", \"precision\", \"f-score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
