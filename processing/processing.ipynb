{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tokenizer import tokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.bleicorpus import BleiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load args\n",
    "with open(\"../args.json\", \"r\") as f:\n",
    "    args = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load documents\n",
    "if args[\"target_data\"]!=\"\":\n",
    "    # load data\n",
    "    df = pd.read_csv(args[\"raw_data\"], index_col = 'id_prose',\n",
    "    usecols = ['id_prose', 'sin_fecha_siniestro', 'sin_relato'], sep=',')\n",
    "    # change data type\n",
    "    df['sin_fecha_siniestro'] = pd.to_datetime(df['sin_fecha_siniestro'])\n",
    "    # sort by date\n",
    "    df.sort_values('sin_fecha_siniestro', inplace=True)\n",
    "    # select data between 2011-2016 and skip nulls\n",
    "    df = df[(df['sin_fecha_siniestro']>=pd.Timestamp(2011,1,1)) \n",
    "        & (df['sin_fecha_siniestro']<pd.Timestamp(2017,1,1)) \n",
    "        & (df['sin_relato'].isnull()==False)]\n",
    "    # export data\n",
    "    path_to_export = args[\"raw_data\"].split(\".csv\")[0]+\".pkl\"\n",
    "    df.to_pickle(path_to_export) \n",
    "else:\n",
    "    # load data\n",
    "    df = pd.read_pickle(args[\"target_data\"])\n",
    "\n",
    "# load stopwords\n",
    "if args[\"stopwords\"]!=\"\":\n",
    "    with open(args[\"stopwords\"], \"r\") as f:\n",
    "        stopwords = [line.strip() for line in f]\n",
    "else:\n",
    "    stopwords = None\n",
    "\n",
    "# load dicionary with homologations\n",
    "if args[\"homol_dict\"]!=\"\":\n",
    "    with open(args[\"homol_dict\"], \"r\") as f:\n",
    "        homol_dict = json.load(f)\n",
    "else:\n",
    "    homol_dict = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"***Data Processing***\")\n",
    "\n",
    "def split_docs(df, slice_type):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        df: pandas.DataFrame, corpus dataframe.\n",
    "        slice_type: str, type of temporal division of the corpus ('year', 'quarter' or 'month').\n",
    "    Output\n",
    "        df_slices: pandas.DataFrame, dataframe with slice assignment for each pair of (year, month).\n",
    "    \"\"\"\n",
    "    df_slices = df[[\"year\", \"month\"]].drop_duplicates()\n",
    "    df_slices.reset_index(inplace=True, drop=True)\n",
    "    df_slices[\"slice\"] = 0\n",
    "    N = len(df_slices)\n",
    "    if slice_type == \"year\":\n",
    "        steps = 12\n",
    "        M = int(N/steps)\n",
    "        slices = range(1, M+1)\n",
    "    elif slice_type == \"quarter\":\n",
    "        steps = 3\n",
    "        M = int(N/steps)\n",
    "        slices = range(1, M+1)\n",
    "    else:\n",
    "        # slice_type == \"month\"\n",
    "        steps = 1\n",
    "        slices = range(1, N+1)\n",
    "\n",
    "    for slice in slices:\n",
    "        df_slices.loc[steps*(slice-1):steps*slice, \"slice\"] = slice\n",
    "\n",
    "    return df_slices\n",
    "\n",
    "# group corpus into slices\n",
    "df.loc[:, \"year\"] = df.apply(lambda x: x[\"sin_fecha_siniestro\"].year, axis=1)\n",
    "df.loc[:, \"month\"] = df.apply(lambda x: x[\"sin_fecha_siniestro\"].month, axis=1)\n",
    "df.loc[:, \"slice\"] = 0\n",
    "df_slices = split_docs(df, args[\"slice_type\"])\n",
    "slices = df_slices[\"slice\"].unique()\n",
    "\n",
    "for row in df_slices.values: \n",
    "    year, month, slice = row[0], row[1], row[2]\n",
    "    df.loc[(df[\"year\"]==year) & (df[\"month\"]==month), \"slice\"] = slice\n",
    "\n",
    "# make folder to export results\n",
    "path_to_save = f'{args[\"corpus\"]}{args[\"slice_type\"]}/'\n",
    "if os.path.exists(path_to_save):\n",
    "    # remove pre executions\n",
    "    shutil.rmtree(path_to_save)\n",
    "os.makedirs(path_to_save)\n",
    "\n",
    "for slice in slices:\n",
    "    logger.info(f\"***Slices Completed:{slice-1}/{slices[-1]}***\")\n",
    "    \n",
    "    docs = df[df[\"slice\"] == slice][\"sin_relato\"]\n",
    "    logger.info(f\"Corpus size: {len(docs)}\")\n",
    "\n",
    "    logger.info(\"Extracting Vocabulary\")\n",
    "\n",
    "    tokenizer_args = {\"stopwords\": stopwords, \"homol_dict\": homol_dict, \n",
    "                      \"stemming\": args[\"stemming\"], \"lemmatization\": args[\"lemmatization\"]}\n",
    "    tf_vectorizer = CountVectorizer(analyzer='word', tokenizer=lambda text: tokenizer(text, **tokenizer_args))\n",
    "    tf_vectorizer.fit(docs)\n",
    "    vocabulary = tf_vectorizer.get_feature_names()\n",
    "    frequency = tf_vectorizer.transform(docs).toarray().sum(axis=0)\n",
    "    word_freq = {'vocabulary':pd.Series(vocabulary), 'frequency': pd.Series(frequency)}\n",
    "    df_word_freq = pd.DataFrame(word_freq)\n",
    "\n",
    "    logger.info(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "    # remove words with less frequency and with little chars\n",
    "    vocabulary = df_word_freq[(df_word_freq['frequency']>=2) & (df_word_freq['vocabulary'].str.len()>3)]['vocabulary'].to_list()\n",
    "    logger.info(f\"Vocabulary size after elimination: {len(vocabulary)}\")\n",
    "\n",
    "    logger.info(\"Getting corpus in Bleiâ€™s LDA-C format\")\n",
    "\n",
    "    # tokenized corpus using vocabulary\n",
    "    tokenizer_args = {\"stopwords\": stopwords, \"homol_dict\": homol_dict, \"vocabulary\": vocabulary, \n",
    "                      \"lemmatization\": args[\"lemmatization\"], \"stemming\": args[\"stemming\"]}\n",
    "    corpus = [tokenizer(doc, **tokenizer_args) for doc in docs]\n",
    "    # remove empty documents\n",
    "    corpus = [doc for doc in corpus if len(doc)>0]\n",
    "\n",
    "    logger.info(f\"Corpus size after elimination of empty docs: {len(corpus)}\")\n",
    "\n",
    "    # map each word to an id {id->word}\n",
    "    dictionary = Dictionary(corpus)\n",
    "    # a document is a list of tuples, each tuples has two element, the first is the id of the word and the second is its frequency\n",
    "    corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "\n",
    "    logger.info(\"Saving Corpus\")\n",
    "    # save dictionary and corpus \n",
    "    zeros = \"0\"*(len(str(slices[-1]))-len(str(slice)))\n",
    "    slice_string = f\"{zeros}{slice}\"\n",
    "    dictionary.save(f\"{path_to_save}dictionary_{slice_string}.dict\")\n",
    "    BleiCorpus.serialize(f\"{path_to_save}corpus_{slice_string}.mm\", corpus)\n",
    "\n",
    "logger.info(\"***Data Processing Completed***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Innovation rate\n",
    "\n",
    "How much change the vocabulary between adjacent epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dictionaries\n",
    "path_corpus = f'{args[\"corpus\"]}{args[\"slice_type\"]}'\n",
    "dict_files = sorted([file for file in os.listdir(path_corpus) if \".dict\" in file])\n",
    "dictionaries = {}\n",
    "slices = range(1, len(dict_files)+1)\n",
    "for slice in slices:\n",
    "    path_dict = f'{path_corpus}/{dict_files[slice-1]}'\n",
    "    token2id = Dictionary.load(path_dict).token2id\n",
    "    dictionaries[slice] = token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "innovation_data = []\n",
    "for slice in slices[:-1]:\n",
    "    vocabulary1 = set(dictionaries[slice].keys())\n",
    "    vocabulary2= set(dictionaries[slice+1].keys())\n",
    "    \n",
    "    old_vocabulary = len(vocabulary1)\n",
    "    new_vocabulary = len(vocabulary2)\n",
    "    \n",
    "    old_words = round(100*len(vocabulary1-vocabulary2)/old_vocabulary, 2)\n",
    "    new_words = round(100*len(vocabulary2-vocabulary1)/old_vocabulary, 2)\n",
    "    vocabulary_change = {\"slice\":slice+1, \"old_vocabulary\": old_vocabulary, \"new_vocabulary\": new_vocabulary, \n",
    "     \"%old_words\": old_words, \"%new_words\":new_words}\n",
    "    innovation_data.append(vocabulary_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice</th>\n",
       "      <th>old_vocabulary</th>\n",
       "      <th>new_vocabulary</th>\n",
       "      <th>%old_words</th>\n",
       "      <th>%new_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1314</td>\n",
       "      <td>1418</td>\n",
       "      <td>28.23</td>\n",
       "      <td>36.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1418</td>\n",
       "      <td>1497</td>\n",
       "      <td>28.84</td>\n",
       "      <td>34.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1497</td>\n",
       "      <td>1551</td>\n",
       "      <td>30.33</td>\n",
       "      <td>33.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1551</td>\n",
       "      <td>1457</td>\n",
       "      <td>33.72</td>\n",
       "      <td>27.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1457</td>\n",
       "      <td>1507</td>\n",
       "      <td>30.95</td>\n",
       "      <td>34.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1507</td>\n",
       "      <td>1514</td>\n",
       "      <td>31.52</td>\n",
       "      <td>31.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1514</td>\n",
       "      <td>1498</td>\n",
       "      <td>32.43</td>\n",
       "      <td>31.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1498</td>\n",
       "      <td>1496</td>\n",
       "      <td>32.71</td>\n",
       "      <td>32.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1496</td>\n",
       "      <td>1722</td>\n",
       "      <td>27.87</td>\n",
       "      <td>42.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1722</td>\n",
       "      <td>1573</td>\n",
       "      <td>36.53</td>\n",
       "      <td>27.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>1573</td>\n",
       "      <td>1605</td>\n",
       "      <td>33.38</td>\n",
       "      <td>35.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1605</td>\n",
       "      <td>1686</td>\n",
       "      <td>30.47</td>\n",
       "      <td>35.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1686</td>\n",
       "      <td>1775</td>\n",
       "      <td>29.95</td>\n",
       "      <td>35.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>1775</td>\n",
       "      <td>1738</td>\n",
       "      <td>32.06</td>\n",
       "      <td>29.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>1738</td>\n",
       "      <td>1809</td>\n",
       "      <td>29.40</td>\n",
       "      <td>33.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>1809</td>\n",
       "      <td>1772</td>\n",
       "      <td>32.17</td>\n",
       "      <td>30.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1772</td>\n",
       "      <td>1873</td>\n",
       "      <td>27.54</td>\n",
       "      <td>33.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>1873</td>\n",
       "      <td>1926</td>\n",
       "      <td>28.72</td>\n",
       "      <td>31.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>1926</td>\n",
       "      <td>1785</td>\n",
       "      <td>32.97</td>\n",
       "      <td>25.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>1785</td>\n",
       "      <td>1824</td>\n",
       "      <td>30.31</td>\n",
       "      <td>32.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>1824</td>\n",
       "      <td>1882</td>\n",
       "      <td>29.44</td>\n",
       "      <td>32.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>1882</td>\n",
       "      <td>1896</td>\n",
       "      <td>29.86</td>\n",
       "      <td>30.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>1896</td>\n",
       "      <td>1936</td>\n",
       "      <td>29.96</td>\n",
       "      <td>32.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    slice  old_vocabulary  new_vocabulary  %old_words  %new_words\n",
       "0       2            1314            1418       28.23       36.15\n",
       "1       3            1418            1497       28.84       34.41\n",
       "2       4            1497            1551       30.33       33.93\n",
       "3       5            1551            1457       33.72       27.66\n",
       "4       6            1457            1507       30.95       34.39\n",
       "5       7            1507            1514       31.52       31.98\n",
       "6       8            1514            1498       32.43       31.37\n",
       "7       9            1498            1496       32.71       32.58\n",
       "8      10            1496            1722       27.87       42.98\n",
       "9      11            1722            1573       36.53       27.87\n",
       "10     12            1573            1605       33.38       35.41\n",
       "11     13            1605            1686       30.47       35.51\n",
       "12     14            1686            1775       29.95       35.23\n",
       "13     15            1775            1738       32.06       29.97\n",
       "14     16            1738            1809       29.40       33.49\n",
       "15     17            1809            1772       32.17       30.13\n",
       "16     18            1772            1873       27.54       33.24\n",
       "17     19            1873            1926       28.72       31.55\n",
       "18     20            1926            1785       32.97       25.65\n",
       "19     21            1785            1824       30.31       32.49\n",
       "20     22            1824            1882       29.44       32.62\n",
       "21     23            1882            1896       29.86       30.61\n",
       "22     24            1896            1936       29.96       32.07"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innovation_rate = pd.DataFrame(innovation_data)\n",
    "innovation_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice</th>\n",
       "      <th>old_vocabulary</th>\n",
       "      <th>new_vocabulary</th>\n",
       "      <th>%old_words</th>\n",
       "      <th>%new_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>1657.304348</td>\n",
       "      <td>1684.347826</td>\n",
       "      <td>30.841739</td>\n",
       "      <td>32.664783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>6.78233</td>\n",
       "      <td>176.896332</td>\n",
       "      <td>169.413858</td>\n",
       "      <td>2.170793</td>\n",
       "      <td>3.492412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>27.540000</td>\n",
       "      <td>25.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>7.50000</td>\n",
       "      <td>1502.500000</td>\n",
       "      <td>1510.500000</td>\n",
       "      <td>29.420000</td>\n",
       "      <td>30.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>1686.000000</td>\n",
       "      <td>1722.000000</td>\n",
       "      <td>30.330000</td>\n",
       "      <td>32.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>18.50000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1816.500000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>34.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>1926.000000</td>\n",
       "      <td>1936.000000</td>\n",
       "      <td>36.530000</td>\n",
       "      <td>42.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          slice  old_vocabulary  new_vocabulary  %old_words  %new_words\n",
       "count  23.00000       23.000000       23.000000   23.000000   23.000000\n",
       "mean   13.00000     1657.304348     1684.347826   30.841739   32.664783\n",
       "std     6.78233      176.896332      169.413858    2.170793    3.492412\n",
       "min     2.00000     1314.000000     1418.000000   27.540000   25.650000\n",
       "25%     7.50000     1502.500000     1510.500000   29.420000   30.990000\n",
       "50%    13.00000     1686.000000     1722.000000   30.330000   32.580000\n",
       "75%    18.50000     1797.000000     1816.500000   32.300000   34.400000\n",
       "max    24.00000     1926.000000     1936.000000   36.530000   42.980000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innovation_rate.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Extraer stopwords contextuales: palabras muy frecuentes que aportan poca informaciÃ³n\n",
    "stopwords_filter = ['veh', 'vh', 'culo', 'aut', 'camion', 'camiÃ³n','rob', 'denun', 'dej', \n",
    "                    'daÃ±o', 'dano', 'daos', 'perc' ,'sinies' ,'llev' ,'volver' ,'sali' ,\n",
    "                    'hrs' ,'dedu' ,'hecho' , 'habia', 'busc' ,'regre' ,'aseg' ,\n",
    "                    'frent' ,'comuna' ,'direcc' ,'aprox' ,'circun' ,'lleg' , 'afuera' ,\n",
    "                    'hora' ,'indica' ,'ubica' ,'minut' ,'conta', 'presen', 'senal', \n",
    "                    'seÃ±al', 'web', 'descrip', 'carabi', 'fech', 'avis', 'docu', 'ppu',\n",
    "                    'dya', 'mario', 'medina', 'alcoholemiano'\n",
    "                   ]\n",
    "#ppu:patente, web: pÃ¡gina web para denunciar\n",
    "\n",
    "not_stopwords = set(['autopista', 'autopistas', 'autoservicio', 'automÃ¡tico', 'algarrobo', 'petrobras'])\n",
    "contextual_stopwords = contextual_stopwords-not_stopwords\n",
    "contextual_stopwords = contextual_stopwords.union(set(['via', 'vÃ­a', 'uf', 'numero', 'nÃºmero']))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "Ver como capturar patrÃ³n horario, usando intervalos horarios a partir de nÃºmeros o palabras:\n",
    "- tiempo = [ 'am', 'maÃ±ana', 'manana', 'pm', 'noche', 'madrugada', 'anoche']\n",
    "\n",
    "Crear diccionario de homologaciones. Ejemplo:\n",
    "- estacionar, bajar\n",
    "- arma_fuego: [pistola, arma de fuego, ..]\n",
    "- sujetos, tipos, individiuos, persona, tipo, personas bajar\n",
    "\n",
    "Trabajar en la generaciÃ³n de frases, ejemplo:\n",
    "- cajero_automÃ¡tico = [cajero automÃ¡tico]\n",
    "- estacion_central = ['estaciÃ³n central', 'estacion central']\n",
    "- estacion_de_servicio = ['estaciÃ³n de servicio', 'estacion de servicio', 'autoservicio', 'estacion de autoservicio ']\n",
    "- estacion_del_metro = ['estaciÃ³n del metro']\n",
    "\n",
    "Consideraciones\n",
    "- CorreciÃ³n ortogrÃ¡fica: embeddings (60 ms) y permutaciones (230 ms).\n",
    "- LematizaciÃ³n: spacy (5 ms), CREA (137 ns), se cae con faltas ortogrÃ¡ficas\n",
    "- Palabras cortadas: realizar bÃºsqueda exhaustiva de carÃ¡cteres que cortan palabras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
