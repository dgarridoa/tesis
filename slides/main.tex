% Template:     Presentación LaTeX
% Documento:    Archivo principal
% Versión:      1.4.5 (27/09/2021)
% Codificación: UTF-8
%
% Autor: Pablo Pizarro R.
%        pablo@ppizarror.com
%
% Manual template: [https://latex.ppizarror.com/presentacion]
% Licencia MIT:    [https://opensource.org/licenses/MIT]

% CREACIÓN DEL DOCUMENTO
\pdfminorversion=7
\documentclass[
	spanish, % Idioma: spanish, english, etc.
	aspectratio=43, % 1610, 169, 149, 54, 43, 32
	hyperref={pdfencoding=auto,psdextra},
	xcolor={dvipsnames,table,usenames},
]{beamer}

% INFORMACIÓN DEL DOCUMENTO
\def\documenttitle {Modelamiento y seguimiento de tópicos para detección de modus operandi en robo de vehículos}
\def\documentsubtitle {Modelamiento dinámico de tópicos}
\def\documentsubject {Tesis para optar al grado de Magíster en Gestión de Operaciones \newline Memoria para optar al título de Ingenierio Civil Industrial}

\def\documentauthor {Diego Garrido}
\def\coursename {}
\def\coursecode {}

\def\universityname {Universidad de Chile}
\def\universityfaculty {Facultad de Ciencias Físicas y Matemáticas}
\def\universitydepartment {Departamento de Ingeniería Industrial}
\def\universitydepartmentimage {departamentos/fcfm}
\def\universitylocation {Santiago de Chile}

% CONFIGURACIÓN DATOS BEAMER
\title[\documentsubtitle]{\documenttitle}
\subtitle{\documentsubject}
\author[\documentauthor]{
	\documentauthor \newline\newline
Profesor guía: Richard Weber \newline
Miembros de la comisión: Giorgiogiulio Parra, Ángel Jiménez
}
\institute[UChile]{
	\includegraphics[height=1.1cm]{\universitydepartmentimage} \\
	\medskip
	\universityname \\
	\universityfaculty \\
	\universitydepartment
}
\date[\today]{\footnotesize{\today}}

% IMPORTACIÓN DEL TEMPLATE
\input{template}

% INICIO DE LAS PÁGINAS
\begin{document}

% CONFIGURACIÓN DE PÁGINA Y ENCABEZADOS
\templatePagecfg

% CONFIGURACIONES FINALES
\templateFinalcfg

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\definecolor{royalblue}{HTML}{214ED3}
% ======================= INICIO DEL DOCUMENTO =======================
\begin{frame}
	\titlepage
\end{frame}

\begin{frame}
	\frametitle{Contenidos}
	\tableofcontents
\end{frame}
\section{Motivación}
\begin{frame}[t]
\frametitle{Motivación} 
% agregar dos imagenes que representen el punto 1 y 3. ejemplo de accidentes de trayecto causado por mascarillas.
\begin{columns}[t]
  \column{0.5\textwidth}
  \begin{center}
    \begin{minipage}{0.8\textwidth}
    \begin{itemize}
    \item[] \textbf{\scalebox{1.5}{\textcolor{royalblue}{2 EB}} de data por día} %10^18
    \item[] \textbf{\scalebox{1.5}{\textcolor{royalblue}{80\%}} no estructurado}
    \item[] \textbf{\scalebox{1.5}{\textcolor{royalblue}{12\%}} es analizada}
    \end{itemize}
    \end{minipage}
   \end{center}
  \blfootnote{\url{https://www.sigmacomputing.com/blog/top-20-big-data-statistics/}}
  
  \vspace*{-0.2in}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{covid_mask.jpg}
  \end{figure}

  \vspace*{-0.4in}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{similarity_graph.png} 
  \end{figure}

\column{0.5\textwidth}
  \begin{itemize}
    \item Aumento del volumenes de textos requiere métodos automáticos de procesamientos. 
    \item El modelamiento de tópicos dinámico permite descubrir nuevos fenómenos de base. Ejemplo: descubrimiento de nuevos tipos de accidentes de trayecto apartir de relatos de accidentes laborales.
    \item Este trabajo es una propuesta de modelamiento dinámico de nacimiento, muerte, evolución, división y fusión de tópicos.
  \end{itemize}
\end{columns}
\end{frame}

\section{Revisión del estado del arte}

\begin{frame}[t]
\frametitle{Revisión del estado del arte: ¿Qué es el modelamiento de tópicos?}
  El modelamiento de tópicos es uno de los enfoques más prometores de \textit{clustering} aplicado a texto, siendo su objetivo descubrir los temas (\textit{clusters}) ocultos presentes en el corpus, permitiendo \textbf{resumir}, \textbf{organizar} y \textbf{explorar} grandes colecciones de datos.

\insertimage{topic_modelling.png}{scale=0.3}{}

%El problema enunciado consiste en una tarea \textit{clustering}, debido a que no se cuenta con una etiqueta del tema al que corresponde cada documento, siendo el propósito del trabajo descubrirla. El modelamiento de tópicos es uno de los enfoques más prometores de \texit{clustering} aplicado a texto, siendo su objetivo descubrir los temas (\textit{clusters}) ocultos presentes en el corpus, permitiendo resumir, organizar y explorar grandes colecciones de datos.\\
\end{frame}

\begin{frame}[t]
\frametitle{Revisión del estado del arte: Tipos de modelos de tópicos}
Las técnicas de modelamiento de tópicos suelen estar basadas en \textbf{factorización matricial} o en \textbf{modelos probabilísticos generativos}. 
\newline\newline
A continuación algunos ejemplos de ambos enfoques:

%reemplazar por monos
\begin{itemize}
  \item \textbf{LSI} (Latent Semantic Indexing) \cite{dumais2004latent} o \textbf{NMF} (Non-negative Matrix Factorization)\cite{xu2003document}. 
  \item \textbf{LDA} (Latent Dirichlet Allocation)\cite{blei2003latent} o \textbf{HDP} (Hierarchical Dirichlet Process)\cite{teh2005sharing}. 
\end{itemize} 

Este trabajo aborda el enfoque probabilístico: 
\begin{itemize}
  \item \textbf{Expresa incertidumbre} en la asignación de un tópico a un documento y en la asignación de palabras a los tópicos.% Por que es mejor?
  \item Suele aprender \textbf{tópicos más descriptivos} \cite{stevens2012exploring}.% Por que es mas descriptivo?
\end{itemize}

\end{frame}

\begin{frame}[t]
\frametitle{Revisión del estado del arte: Modelamiento dinámico}
En el modelamiento de tópicos se pueden presentar los siguientes dinámismos:

\begin{enumerate}
  \item \textbf{Evolución de tópicos.}
  \item \textbf{Dinámismo en la mezcla de tópicos.}
  \item \textbf{Nacimiento, muerte, fusión y división de tópicos.}
\end{enumerate}

Dentro de los modelos de tópicos dinamicos se tiene:
\begin{itemize}
  \item \textbf{DTM} (Dynamic Topic Modelling)\cite{blei2006dynamic} y \textbf{TOC} (Topic Over Time)\textbf{permiten el punto 1 y 2} manteniendo fijo el número de topicos en el tiempo.
  \item \textbf{DHDP} (Dynamic Hierarchical Dirichlet Process)\cite{ahmed2012timeline} \textbf{captura el punto 1, 2 y 3 parcialmente}, con excepción de fusión y división. \textbf{No cuenta con una implementación}.\\
  \item En \cite{wilson2011tracking} y \cite{beykikhoshk2018discovering} se capturan los dinámismos mencionados dividiendo el corpus en épocas, entrenando de forma independiente un modelo por época para finalmente unificar (LDA y HDP).
\end{itemize}


\end{frame}


\section{Metodología propuesta}
\begin{frame}[t]
\frametitle{Metodología propuesta}
\begin{itemize}
  \item División del corpus en épocas siendo cada época procesada mediante tokenización, eliminación de caracteres, eliminación de \textit{stopwords}, filtro por vocabulario y filtro por frecuencia. 
  \item Aplicación de HDP en cada época de manera independiente. 
  \item Construcción del grafo computando la similitud WMD entre tópicos de épocas adyacentes y eliminación de arcos cuya similitud es menor al cuantil $\zeta$ de la distribución acumulada de la similitud.
\end{itemize}

\vspace*{-0.2in}
\insertimage{methodology.png}{scale=0.18}{}

\end{frame}


\begin{frame}[t]
\frametitle{Metodología propuesta: Hierarchical Dirichlet Process}
\textbf{HDP} (Hierarchical Dirichlet Process) es un \textit{prior} jerárquico no paramétrico, el cual está formado por un DP cuya medida base $G_{0}$ es dibujada a partir de un DP. 
\newline\newline
\begin{columns}
  \begin{column}{0.5\textwith}
  \hspace*{-3cm}\vbox{\begin{aligned}
         H \quad &= \quad \text{Dir}(\frac{\eta}{|V|}1_{|V|})\\
         G_{0}|\gamma, H \quad &\sim \quad \text{DP}(\gamma, H)\\
         G_{d}|\alpha, G_{0} \quad &\sim \quad \text{DP}(\alpha_{0}, G_{0})\\
         \phi_{d,n}|G_{d} \quad &\sim \quad G_{d}\\
         w_{d,n}|\phi_{d,n} \quad &\sim \quad \text{Cat}(\phi_{d,n})
  \end{aligned}}
  \end{column}
  \begin{column}{0.5\textwith}
  \hspace*{-10em}\raisebox{2em}{\scalebox{0.8}{
      \tikz{ %
      \node[latent, dashed] (H) {$H$} ; %
      \node[latent, right=of H] (G0) {$G_{0}$} ; %
      \node[latent, above= of G0, dashed] (gamma) {$\gamma$} ; %
      \node[latent, right=of G0] (Gd) {$G_{d}$} ; %
      \node[latent, above= of Gd, dashed] (alpha0) {$\alpha_{0}$} ; %
      \node[latent, right= of Gd] (phi) {$\phi_{d,n}$} ; %
      \node[obs, right=of phi] (w) {$w_{d,n}$}   ; %
      \plate[inner sep=0.25cm, xshift=-0.12cm, yshift=0.12cm] {plate1} {(phi) (w)} {$N_{d}$}; %
      \plate[inner sep=0.25cm, xshift=-0.12cm, yshift=0.12cm] {plate2} {(Gd) (plate1)} {$D$}; %
      \edge {H, gamma} {G0} ; %
      \edge {G0, alpha0} {Gd} ; %
      \edge {Gd} {phi} ; %
      \edge {phi} {w} ; %
      }
    }}
%  \caption{Representación gráfica de HDP: círculos denotan variables aleatorias, círculos abiertos denotan parámetros, círculos sombreados denotan variables observadas y los platos indican replicación.}
\end{column}
\end{columns}

La discretitud de $G_{0}$ asegura:
  \begin{itemize}
      \item A nivel corpus los documentos comparten el mismo conjunto de tópicos (\textit{mixture components}). 
      \item A nivel documento $G_{d}$ hereda los tópicos de $G_{0}$, pero los pesos de cada tópico (\textit{mixture proportions}) es específica del documento.
  \end{itemize}
\end{frame}
 
\begin{frame}[t]
\frametitle{Metodología propuesta: Grafo de similitud temporal}
\begin{columns}[t]
\column{0.5\textwidth}
  \begin{itemize}
    \item Construcción del grafo \textit{fully connected} de las similitudes entre tópicos de épocas adyacentes ($\phi_{t,i}$ y $\phi_{t+1,j}$) usando una medida de similitud $\rho \in [0,1]$.
    \item Eliminación de las conexiones débiles en base a un umbral $\zeta \in [0,1]$, reteniendo solo aquellas conexiones que cumplen $\rho(\phi_{t,i}, \phi_{t+1,j})\leq \zeta$.

  \end{itemize}
\insertimage{similarity_graph.png}{scale=0.12}{}
		
\column{.5\textwidth} 
  \begin{itemize}
    \item El umbral de corte es el cuantil $\zeta \in [0,1]$ de la cdf de las similitudes ($F_{p}$), es decir, $F_{p}^{-1}(\zeta)$.
    \item El úmbral de corte no es arbitrario según la médida de similitud escógida.
  \end{itemize}
  \insertimage{cdf_sim.png}{scale=0.12}{}
\end{columns}

\end{frame}

\begin{frame}[t]
\frametitle{Metodología propuesta: Word Mover's Distance}
  \begin{itemize}
    \item Se escoge \textbf{WMD} (Word Mover's Distance) \cite{kusner2015word}: distancia que permite comparar vectores sin vocabulario común ya que trabaja sobre el espacio de los\textit{word embeddings}. 
    \item Sea  $V_{i}$ y $V_{j}$ los vocabularios del tópico $i$ y $j$ respectivamente, luego su WMD viene dado por $WMD(\phi_{i}, \phi_{j})$:
  \end{itemize}

\vspace*{-0.3in}
\begin{columns}
\begin{column}{0.4\textwidth}

\begin{align}
\underset{x}{\text{min}}&\sum_{u \in V_{i}}\sum_{v \in V_{j}} c_{u,v}x_{u,v} \\ 
\textrm{s.t.} &\sum_{v \in V_{j}}x_{u,v}= \phi_{i,u}, \; u \in V_{i}\\ 
& \sum_{u \in V_{i}}x_{u,v}= \phi_{j,v}, \; v\in V_{j}\\
& x_{u,v} \geq 0,\; u \in V_{i} \;, v \in V_{j}\\ \nonumber
\end{align}

\end{column}

\begin{column}{0.6\textwidth}
\insertimage{wmd-obama.png}{scale=0.15}{}
\end{column}

\end{columns}
La WMD se puede transformar fácilmente en una médida de similitud considerando $\rho(\phi_{i}, \phi_{j}) = \frac{1}{1+WMD(\phi_{i}, \phi_{j})}$. Notar que si la WMD es 0 la similitud es 1 y si es $\infty$ la similitud es 0. \\

\end{frame}

\begin{frame}[t]
\frametitle{Metodología propuesta: WMD complejidad}

WMD es una medida de distancia \textbf{intensiva en recursos computacionales}.\newline

Usando el algoritmo desarrollado por \cite{pele2009fast} se tiene que el mejor tiempo promedio escala $\mathcal{O}(N^{2}log N)$, donde $N$ es el tamaño del vocabulario entre dos épocas adyacentes.
  \begin{align*}
\{x| Ax=b, x\geq 0\}, A\in \mathbb{R}^{2N\times N^{2}}, b\in \mathbb{R}^{2N}, x\in \mathbb{R}^{N}
  \end{align*}

Se requiere de \textbf{heurísticas} para acelerar el tiempo de computo.
\begin{itemize}
  \item Los tópicos siguen una distribución con forma de \textbf{ley de potencia} sobre el vocabulario, donde una pequeña fracción de las palabras concentran la mayor parte de la masa de la distribución. 
  \item En la práctica \textbf{la interpretación de los tópicos se basa en los top $N$ palabras más probables}, usualmente con $N \in [5, 30]$, entonces, se puede aprovechar esta estructura para efectos de computar la WMD de un forma más eficiente, por ejemplo, utilizando solo las palabras que capturan un determinado porcentaje de la distribución acumulada del tópico.\\
\end{itemize}
\end{frame}

\begin{frame}[t]
\frametitle{Metodología propuesta: Configuracion de hiperparametros}

\begin{columns}[t]
\begin{column}{0.5\textwidth}
HDP cuenta con tres hiperparámertos:
\begin{itemize} 
  \item El \textbf{parámetro de concentración a nivel corpus $\gamma$} y el \textbf{parámetro de concentración a nivel documento $\alpha_{0}$}. En \cite{teh2005sharing} los parámetros de concentración se integran afuera usando un prior \textit{vague gamma} \cite{escobar1995bayesian}. En este caso se utilizó un prior $\Gamma(\alpha=1, \beta=1)$.
  \item El \textbf{parámetro de la medida base Dirichlet $\eta$}. Se prefiere usar $\eta\in (0,1)$ ya que genera distribuciones \textit{sparse} sobre el vocabulario. En este caso se utilizó un punto intermedio, fijando $\eta=0.5$.\\ 
\end{itemize}
%\insertimage{gamma.eps}{scale=0.5}{}
%\caption{Función de densidad de probabilidad (pdf) de una distribución Gamma para diferentes parámetros de forma $\alpha$ y tasa $\beta$.}
\end{column}

\begin{column}{0.5\textwidth}
El grafo temporal cuenta con dos hiperparámetros:
\begin{itemize}
  \item \textbf{$q \in [0,1]$ cuantil de corte de la cdf del tópico}. Se prefieren valores en $[0.8, 0.95]$ ya que conservan el \textit{core} de palabras del tópico y disminuye significativamente el tiempo de cómputo.
  \item \textbf{$\zeta\in[0,1]$ cuantil de corte de la cdf de las similitudes del grafo \texix{fully connected}}. Se prefieren valores en $[0.9, 0.99]$ ya que se conservan aquellas relaciones con alta similitud relativa.
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\section{Descubrimiento de tópicos en robo de vehículos}

\begin{frame}
\frametitle{}
\end{frame}


\begin{frame}
\frametitle{}
\end{frame}


\begin{frame}
\frametitle{}
\end{frame}


\begin{frame}
\frametitle{}
\end{frame}

\begin{frame}
\frametitle{}
\end{frame}

\begin{frame}
\frametitle{}
\end{frame}

\section{Conclusiones y trabajos futuros}

\begin{frame}
\frametitle{}
\end{frame}

\begin{frame}
\frametitle{}
\end{frame}

%analizar el grafo variacional estudiando los puntos de corte (backbone).
\begin{frame}[allowframebreaks]\normalsize
	\frametitle{\namereferences}
	\bibliography{library}
\end{frame}

\end{document}
