{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de resultados \n",
    "\n",
    "El siguiente notebook se encarga de computar métricas de desempeño sobre los DTM entrenados y genera visualización para la comprensión de los tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pylab as plt \n",
    "import seaborn as sb \n",
    "import json  \n",
    "import os\n",
    "from time import time \n",
    "import datetime as dt\n",
    "import re\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "#tópicos\n",
    "import gensim\n",
    "import pyLDAvis \n",
    "from gensim.models.ldaseqmodel import LdaSeqModel\n",
    "from gensim.models.wrappers import DtmModel\n",
    "\n",
    "#metricas\n",
    "from gensim.matutils import cossim\n",
    "from itertools import combinations\n",
    "# from dit.divergences import jensen_shannon_divergence\n",
    "# from dit import ScalarDistribution\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora.mmcorpus import MmCorpus\n",
    "\n",
    "#Para no mostrar warnings\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = MmCorpus('corpora.mm')\n",
    "dictionary = Dictionary.load(\"dictionary.dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary\n",
    "corpora.mm.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar modelos\n",
    "\n",
    "model = []\n",
    "model_name = []\n",
    "K = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for k in K:\n",
    "    model.append(DtmModel.load('dtm_{k}.model'.format(k=k)))\n",
    "    model_name.append('dtm_{k}.model'.format(k=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_slices = model[0].time_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Entropy\n",
    "\n",
    "\n",
    "$\n",
    " Entropy(K) = \\sum^{M}_{m=1} \\sum^{K}_{k=1} p(z_{m}=k |d=d_{m}) \\left( \\sum^{N_{m}}_{n=1}-p_{t,k,m} \\text{ln } p_{t,k,m} \\right)\n",
    "$, $\\quad \\quad p_{t,k,m} =  \\frac{p(w_{n}=t|z_{n}=k)}{\\sum^{V_{m}}_{n=1}p(w_{n}=t|z_{n}=k)}$\n",
    "\n",
    "http://www.mdpi.com/1099-4300/19/5/173/htm\n",
    "## 1.2 Perplexity\n",
    "\n",
    "$\n",
    "    Perplexity(K) = 2^{Entropy(K)}\n",
    "$\n",
    "\n",
    "Mayor entropia indica que mayor es la incertidumbre sobre una variable aleatoria, por tanto se desea minimizar, además la entropía es proporcional a la negative log-likelihood, la cual es positiva y se desea minimizar. La perplexity es una función monótona (debido a que es proporcional a la NLL), en particular decreciente, por lo que a mayor número de tópicos más pequeña es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones para obtener la entropía\n",
    "\n",
    "def second_term(k, m, D, dictionary, dict_ptk):\n",
    "    \n",
    "        #Lista de probabilidades de cada palabra del documento m, con repeticion de palabras\n",
    "        N_m = [dictionary[D[m][n][0]] for n in range(len(D[m])) for f in range(D[m][n][1])]\n",
    "\n",
    "        #Lista de probabilidades de cada palabra del documento m, pero sin repetir palabras\n",
    "        V_m = [dictionary[D[m][n][0]] for n in range(len(D[m]))]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Suma de las probabilidades de cada palabra del documento m, pero sin repetir palabras, p[w_n=t|z_n=k] = dict_ptk[k][w]\n",
    "        den = sum([dict_ptk[k][w] for w in V_m])\n",
    "    \n",
    "        return sum([(-dict_ptk[k][w]/den)*np.log(dict_ptk[k][w]/den) for w in N_m])\n",
    "\n",
    "\n",
    "def entropy(time, model):\n",
    "    time_slices = model.time_slices\n",
    "    ntopic = model.num_topics\n",
    "    voc = []\n",
    "    M = time_slices[time] #número de docs en el slice correspondiente\n",
    "    V = model.num_terms #largo del vocabulario\n",
    "    D = corpus[time_slices[time-1]: time_slices[time]+time_slices[time-1]] #corpus del slice escogido\n",
    "\n",
    "    \n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(ntopic):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(ntopic)]\n",
    "        \n",
    "    doc_topics = model.dtm_vis(time=time, corpus=corpus)[0]\n",
    "    \n",
    "    entropy = sum([doc_topics[m, k]*second_term(k, m, D, dictionary, dict_ptk) for m in range(M) for k in range(ntopic)])\n",
    "      \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5430"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].num_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f560a52244bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_slices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-6b28e43a1c1f>\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(time, model)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#número de docs en el slice correspondiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;31m#largo del vocabulario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtime_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtime_slices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#corpus del slice escogido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "#Calcular la entropía de todos los modelos\n",
    "time_slices = model[0].time_slices\n",
    "e = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        e[k, t] = entropy(t, model[k])\n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guardar los valores de  entropia para cada modelo en un .js\n",
    "entropy = [int(e[i].sum()) for i in range(len(e))]\n",
    "filename = \"dtm_html/entropy_data.js\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var entropy_data = {s}\".format(s = entropy))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Average cosine similarity between topic distribution\n",
    "\n",
    "Cao et al. (2009), estima la similitud coseno promedio entre las distribuciones de los tópicos $\\overrightarrow{\\phi_{i}}$, $\\overrightarrow{\\phi_{j}}$ ($ i \\neq j$) y escoge el valor de $K$\n",
    "que minimiza esta cantidad.\n",
    "\n",
    "La similitud coseno calcula la norma L2 del producto punto de dos vectores. Esto es, si $x$ e $y$ son vectores, su similitud coseno es definida como:\n",
    "\n",
    "$$cos(x, y) = \\frac{xy^{T}}{||x||||y||}$$<br>\n",
    "\n",
    "Luego la similitud coseno promedio entre las distribuciones de los tópicos queda definida como sigue:\n",
    "$$avg\\_cosine\\_distance = \\frac{\\sum^{K}_{i=0}\\sum^{K}_{j=i+1}cos(\\phi_{i},\\phi_{j})}{K \\times(K-1)/2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_cosine_distance(time, model): \n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)]\n",
    "    \n",
    "    sum_cosine_distance = 0\n",
    "    \n",
    "    #Calculo la similitud para todas las combinaciones de tópicos\n",
    "    pairs = [*combinations(range(K-1), 2)]\n",
    "    for p in pairs :\n",
    "            sum_cosine_distance+= cossim(dict_ptk[p[0]], dict_ptk[p[1]])\n",
    "            \n",
    "    avg_cosine_distance = sum_cosine_distance/len(pairs)\n",
    "    return avg_cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3347764015197754\n",
      "2.3118460178375244\n",
      "4.08141565322876\n",
      "5.890650272369385\n",
      "8.021163702011108\n",
      "11.037850618362427\n",
      "13.540852308273315\n",
      "19.081740140914917\n",
      "20.48908257484436\n"
     ]
    }
   ],
   "source": [
    "#Calcular la avg_cosine_distance de todos los modelos en todos sus períodos\n",
    "avg_cossim = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_cossim[k, t] = avg_cosine_distance(time=t, model=model[k]) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_cosine_distance para cada modelo en un js\n",
    "avgcossim = [round(avg_cossim[i].mean(), 2) for i in range(len(avg_cossim))]\n",
    "filename = \"dtm_html/avgcossim_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgcossim_data = {s}\".format(s = avgcossim))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Average Jensen-Shannon distance between topic distribution\n",
    "\n",
    "\n",
    "Deveaud et al. (2014) máximiza la distancia \\textit{Jensen-Shannon} promedio entre las distribuciones de tópicos  $\\overrightarrow{\\phi_{i}}$, $\\overrightarrow{\\phi_{j}}$ ($ i \\neq j$), muy similiar a Cao et al. (2009).\n",
    "\n",
    "$$avg\\_js\\_distance = \\frac{\\sum^{K}_{i=0}\\sum^{K}_{j=i+1}D(\\phi_{i},\\phi_{j})}{K \\times(K-1)/2 }$$\n",
    "\n",
    "\n",
    "Donde $D$ es la divergencia de Jensen-Shannon, cuantifica cuán \"distinguibles\" son dos o más distribuciones entre sí. En su forma básica es:\n",
    "\n",
    "$$D(X||Y) = \\frac{H(X+Y)}{2}+\\frac{H(X)+H(Y)}{2}$$\n",
    "\n",
    "Donde $H$ es la entropía, la entropía de una variable aleatoria $X$ con posibles valores ${x_{1}, .., x_{n}}$, se define de la siguiente forma:\n",
    "\n",
    "$$H(X) = -\\sum^{n}_{i=1}P(x_{i})log(P(x_{i}) = \\sum^{V}_{w=1} \\phi^{i}_{w}log(\\phi^{i}_{w}), \\forall i \\in {1, ..., K}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_js_distance(time, model):\n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)]\n",
    "    \n",
    "    sum_js_distance = 0\n",
    "        \n",
    "    #Calculo la similitud para todas las combinaciones de tópicos\n",
    "    pairs = [*combinations(range(K-1), 2)]\n",
    "    for p in pairs :\n",
    "            X = ScalarDistribution([*dict_ptk[p[0]].keys()], [*dict_ptk[p[0]].values()])\n",
    "            Y = ScalarDistribution([*dict_ptk[p[1]].keys()], [*dict_ptk[p[1]].values()])\n",
    "            \n",
    "            sum_js_distance+= jensen_shannon_divergence([X, Y])\n",
    "            \n",
    "    avg_js_distance = sum_js_distance/len(pairs)  \n",
    "    return avg_js_distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.86623668670654\n",
      "249.50126361846924\n",
      "473.1665823459625\n",
      "779.6344659328461\n",
      "1494.8150265216827\n",
      "3610.90580368042\n",
      "4598.188099384308\n",
      "3278.9420113563538\n",
      "3702.1329233646393\n"
     ]
    }
   ],
   "source": [
    "#Calcular la avg_js_distance de todos los modelos en todos sus períodos\n",
    "avg_js = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_js[k, t] = avg_js_distance(time=t, model=model[k]) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_js_distance para cada modelo en un js\n",
    "avgjs= [round(avg_js[i].mean(),2) for i in range(len(avg_js))]\n",
    "filename = \"dtm_html/avgjs_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgjs_data = {s}\".format(s = avgjs))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Arun et al. (2010)\n",
    "\n",
    "\n",
    "Arun et al. (2010) minimiza la Symetric Kullback-Liebler divergence (Jensen-Shannon distance) entre los valores singulares de la representación matricial de las probabilidades de cada tópico ($\\beta$) y la distribución de tópicos ($\\theta$) dentros del corpus.\n",
    "\n",
    "\n",
    "$$Arun\\_Measure(M1,M2) = KL(CM1||CM2) + KL(CM2||CM1) $$\n",
    "\n",
    "Donde, <br>\n",
    "$CM1$ es la distribución de los valores singulares de la topic-word matriz M1, <br>\n",
    "$CM2$ es la distribución obtenida normalizado por el vector $L∗M2$ (donde $L$ es <br> un vector de dimensión $D$  que indica el largo de cada documento en el corpus y $M2$ es la matriz topic-document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_arun(time, model): \n",
    "    K = model.num_topics\n",
    "    voc = []\n",
    "    V = len(dictionary) #largo del vocabulario\n",
    "\n",
    "    #lista de diccionarios de probabilidades, \n",
    "    #cada diccionario tiene como key una palabra del vocabulario y su probabilidad para cada topico en el slice especificado\n",
    "    for k in range(K):\n",
    "        #voc.append(model.print_topic(time = time, topic=k, top_terms= V))\n",
    "        voc.append(model.show_topic(time = time, topicid=k, topn= V))\n",
    "    dict_ptk = [dict((key, value) for value, key in voc[k]) for k in range(K)] # topic distribution\n",
    "    #Pasar la distribucion de los tópicos al formato de la liberia que calcula la metrica\n",
    "    d = pd.DataFrame(dict_ptk) # topic distribution\n",
    "    topic_word_distrib = np.array(d)\n",
    "    \n",
    "    doc_topics = model.dtm_vis(time=time, corpus=corpus)[0] #mezcla de tópicos de los documentos\n",
    "    slices = time\n",
    "    # mezcla de tópicos de los documentos para el slice de evaluacion\n",
    "    doc_topic_distrib = np.array(doc_topics[sum(time_slices[0:time]):sum(time_slices[0:time+1])]) \n",
    "    \n",
    "    #largo de cada documento en terminos de palabras\n",
    "    c = corpus[sum(time_slices[0:time]):sum(time_slices[0:time+1])]\n",
    "    doc_len=[]\n",
    "    for i in range(len(c)):\n",
    "        s = 0\n",
    "        for j in range(len(c[i])):\n",
    "            s += c[i][j][1]\n",
    "        doc_len.append(s)\n",
    "        \n",
    "    doc_lengths = np.array([doc_len])\n",
    "    \n",
    "    return metric_arun_2010(topic_word_distrib, doc_topic_distrib, doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular la avg_js_distance de todos los modelos en todos sus períodos\n",
    "avg_arun = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_arun[k, t] = metric_arun(time=t, model=model[k]) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_js_distance para cada modelo en un js\n",
    "avgarun= [round(avg_arun[i].mean(),2) for i in range(len(avg_arun))]\n",
    "filename = \"dtm_html/avgarun_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgarun_data = {s}\".format(s = avgarun))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Griffiths et al. (2010)\n",
    "\n",
    "\n",
    "Griffiths y Syteyvers  (2004) dicen: \"Para un estadístico Bayesiano enfrentado a una elección entre un conjunto de modelos estadísticos, la respuesta natural es calcular el probabilidad posterior de ese conjunto de modelos dado el observado datos.\" El componente clave de esta probabilidad posterior será el probabilidad de los datos dados el modelo, integrando sobre todos\n",
    "los parámetros en el modelo.<br>\n",
    "\n",
    "En este caso, la data son las palabas en el corpus, $w$, y el modelo es especificado por el número de tópics, $T$, entonces entonces deseamos calcular la probabilidad de palabras para los temas $z$.<br>\n",
    "\n",
    "Como siempre la likelihood es difícil de calcular, entonces esta se puede aproximar $P(w|T)$, tomando la media armónica de un conjunto de valores de $P(z|w, T)$ cuando $z$ es generado de la posterior $P(z|w, T)$. Entonces el algoritmo Gibb sampling provee tales ejemplos, y el valor de $P(w|z, T)$ pude ser computado. Luego elijen el $K$ que maximiza la media ármonica de la log-likelihoods muestreada.<br>\n",
    "\n",
    "\n",
    "Esta métrica sirve para comparar entre modelos con el mismo número de tópicos, a más tópicos es estrictamente decreciente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Coherence score - U mass\n",
    "\n",
    "Mimno et al (2011) propone una métrica para evaluación de la calidad de un tópico. Dado un tópico $z$ y sus top $T$ palabras $V^{(z)}=(v^{(z)}_{1}, ..., v^{(z)}_{T})$ ordenadas por $P(w|z)$, la $coherence score$ es definido como:\n",
    "\n",
    "$$C(z;V^{(z)}) = \\sum^{T}_{t=2}\\sum^{t}_{l=1}log \\frac {D(v^{(z)}_{m}, v^{(z)}_{l})+1}{D(v^{(z)}_{l})}$$\n",
    "\n",
    "Donde $D(v)$ es la frecuencia de la palabra $v$ a nivel documento, $D(v,v^{'})$ es el número de documentos donde la palabra $v$ y $v'$ co-ocurren. El coherence score se basa en la idea que las palabras dentro de un único concepto tenderán a co-ocurrir dentro del mismo documento. Esto esta empíricamente demostrado que el coherence score es altamente correlacionado con la coherencia de tópicos del juicio humano. Debe enfatizarse que el puntaje de coherencia solo es apropiado para medir palabras frecuentes en un tema. Porque la frecuencia de palabras raras es menos confiable.<br>\n",
    "\n",
    "\n",
    "\n",
    "Luego se escoge $k$ que máximiza la coherencia promedio:\n",
    "\n",
    "$$avg\\_coherence\\_score = \\frac{1}{K}\\sum^{K}_{k=1} C(z_{k};V^{(z_{k})})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_score(time, model, topn):\n",
    "    topics_wrapper = model.dtm_coherence(time=time)\n",
    "    cm_wrapper = CoherenceModel(topics=topics_wrapper, corpus=corpus, dictionary=dictionary, coherence='u_mass', topn=topn)\n",
    "    \n",
    "    coherence = cm_wrapper.get_coherence()\n",
    "    \n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.3764705657959\n",
      "24.830121994018555\n",
      "28.218249797821045\n",
      "28.622239112854004\n",
      "35.29431939125061\n",
      "33.817896604537964\n",
      "34.77373766899109\n",
      "33.54385709762573\n",
      "34.262282609939575\n"
     ]
    }
   ],
   "source": [
    "#Calcular la avg_js_distance de todos los modelos en todos sus períodos\n",
    "avg_coherence_score = np.zeros((len(model), len(time_slices)))\n",
    "for k in range(len(model)):\n",
    "    ti = time()\n",
    "    for t in range(len(time_slices)):\n",
    "        avg_coherence_score[k, t] =  coherence_score(time=t, model=model[k], topn=20) \n",
    "    tf = time()\n",
    "    print(tf-ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar los valores de  avg_js_distance para cada modelo en un js\n",
    "avgcoherence= [round(avg_coherence_score[i].mean(),2) for i in range(len(avg_coherence_score))]\n",
    "filename = \"dtm_html/avgcoherence_data.js\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(\"var avgcoherence_data = {s}\".format(s = avgcoherence))\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 LDAVis\n",
    "\n",
    "Esta parte consiste en guardar la data necesaria para realizar las visualizaciones con la librería LDAVis en un diccionario, luego se guarda ese diccionario en una variable en un archivo js que luego será leído por el script a cargo de las visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.39201378822327\n",
      "58.38373398780823\n",
      "60.51923060417175\n",
      "64.82786440849304\n",
      "64.37093257904053\n",
      "68.16515827178955\n",
      "76.49615502357483\n",
      "72.90783071517944\n",
      "80.07492733001709\n"
     ]
    }
   ],
   "source": [
    "slices = time_slices\n",
    "\n",
    "for k in range(len(model)):\n",
    "    t1 = time()\n",
    "    dtm = []\n",
    "    for i in range(len(slices)):\n",
    "        doc_topic, topic_term, doc_lengths, term_frequency, vocab = model[k].dtm_vis(time=i, corpus=corpus)\n",
    "        vis_wrapper = pyLDAvis.prepare(topic_term_dists=topic_term, doc_topic_dists=doc_topic, doc_lengths=doc_lengths, vocab=vocab, term_frequency=term_frequency, sort_topics=False, R=30)\n",
    "        data = pyLDAvis.save_json(vis_wrapper, 'dtm_html/data.js')\n",
    "        with open('dtm_html/data.js') as f:\n",
    "            dtm.append(f.read())\n",
    "\n",
    "    dtm_data = {}\n",
    "    for i in range(len(dtm)):\n",
    "        dtm_data[date[i]] = ast.literal_eval(dtm[i])\n",
    "\n",
    "\n",
    "\n",
    "    dtm_data_json = json.dumps(dtm_data) #parsing\n",
    "\n",
    "    filename = \"dtm_html/dtm.js\"\n",
    "    if not os.path.exists(filename):\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"var dtm_data = {}; \\n\")\n",
    "                f.close()  \n",
    "                \n",
    "                \n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"dtm_data[{ntopics}] = \".format(ntopics=model_name[k].split('.')[0].split('_')[1]) + repr(ast.literal_eval(dtm_data_json)) + \"; \\n\")\n",
    "        f.close()  \n",
    "    t2 = time()\n",
    "    print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Serie de tiempo \n",
    "\n",
    "Esta parte consiste en guardar en un js un diccionario que contiene los tamaños de los tópicos en el tiempo de cada modelo, para luego ser leído por un script que visualiza las series de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>slice</th>\n",
       "      <th>counts</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>475</td>\n",
       "      <td>Jan</td>\n",
       "      <td>Jan 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>302</td>\n",
       "      <td>Feb</td>\n",
       "      <td>Feb 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>660</td>\n",
       "      <td>Mar</td>\n",
       "      <td>Mar 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>544</td>\n",
       "      <td>Apr</td>\n",
       "      <td>Apr 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011</td>\n",
       "      <td>5</td>\n",
       "      <td>634</td>\n",
       "      <td>May</td>\n",
       "      <td>May 2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  slice  counts month      date\n",
       "0  2011      1     475   Jan  Jan 2011\n",
       "1  2011      2     302   Feb  Feb 2011\n",
       "2  2011      3     660   Mar  Mar 2011\n",
       "3  2011      4     544   Apr  Apr 2011\n",
       "4  2011      5     634   May  May 2011"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "dft = df.groupby(['year', 'slice']).size().reset_index(name='counts')\n",
    "dft['month'] = month*6\n",
    "dft['date'] = dft.apply(lambda x: x['month']+' '+str(x['year']), axis=1)\n",
    "date = dft['date'].values.tolist()\n",
    "\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39002013206481934\n",
      "0.40934205055236816\n",
      "0.4426252841949463\n",
      "0.4508399963378906\n",
      "0.4085359573364258\n",
      "0.41791415214538574\n",
      "0.42386627197265625\n",
      "0.44089555740356445\n",
      "0.4330294132232666\n"
     ]
    }
   ],
   "source": [
    "slices = time_slices\n",
    "series = []\n",
    "\n",
    "for j in range(len(model)):\n",
    "    t1 = time()\n",
    "    \n",
    "    doc_topic = model[j].dtm_vis(time=0, corpus=corpus)[0]\n",
    "    doc_topic_dist = doc_topic\n",
    "    doc_topic = [np.argmax(doc_topic_dist[i]) for i in range(len(doc_topic_dist))]\n",
    "\n",
    "    t = time_slices\n",
    "    b = np.array(doc_topic)\n",
    "    u = np.unique(b)\n",
    "\n",
    "    t = time_slices\n",
    "    t = [0]+t\n",
    "    c = [len(b[sum(t[0:i]): sum(t[0:i])+t[i+1]] [b[sum(t[0:i]): sum(t[0:i])+t[i+1]] == v]) for v in u for i in range(len(t)-1)]\n",
    "\n",
    "    \n",
    "    dft = df.groupby(['year', 'slice']).size().reset_index(name='counts')\n",
    "    date = [dt.datetime( year=dft['year'][i] , month=dft['slice'][i], day=1).ctime() for i in range(len(dft))]\n",
    "\n",
    "    d = []\n",
    "    for i in range(len(slices)):\n",
    "        d1 = {}\n",
    "        d1[str(0)] = int(dft['counts'][i])\n",
    "        for k in u:\n",
    "            d1[str(k+1)] = c[len(slices)*k+i]\n",
    "        d1['date'] = date[i]\n",
    "\n",
    "        d.append(d1)\n",
    "    \n",
    "    data_series_json = json.dumps(d) #parsing\n",
    "  \n",
    "    filename = \"dtm_html/series_data.js\"\n",
    "    if not os.path.exists(filename):\n",
    "            with open(filename, \"w\") as f:\n",
    "                f.write(\"var series_data  = {}; \\n\")\n",
    "                f.close()  \n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"series_data[{ntopics}] = \".format(ntopics=model_name[j].split('.')[0].split('_')[1]) + repr(ast.literal_eval(data_series_json)) + \"; \\n\")\n",
    "        f.close()  \n",
    "    t2 = time()\n",
    "    print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este script es para pasarle los datos que genero el script anterior en un formato que la librería que visualiza la serie de tiempo entiende mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029916763305664062\n",
      "0.0009975433349609375\n",
      "0.0009975433349609375\n",
      "0.0009968280792236328\n",
      "0.0009975433349609375\n",
      "0.000997304916381836\n",
      "0.000997304916381836\n",
      "0.0019948482513427734\n",
      "0.0009970664978027344\n"
     ]
    }
   ],
   "source": [
    "filename = \"dtm_html/graph_data.js\"\n",
    "if not os.path.exists(filename):\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"var graph_data  = {}; \\n\")\n",
    "            f.close()  \n",
    "\n",
    "color = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#d2f53c', '#fabebe']\n",
    "for k in K:\n",
    "    t1 = time()\n",
    "    data = []\n",
    "    d = {\"id\": \"g\",\n",
    "         \"valueAxis\": \" \",\n",
    "         \"lineColor\": \"#808080\",\n",
    "         \"bullet\": \"round\",\n",
    "         \"bulletBorderThickness\": 1,\n",
    "         \"hideBulletsCount\": 30,\n",
    "         \"title\": \"Total\",\n",
    "         \"valueField\": \"0\",\n",
    "         \"fillAlphas\": 0}\n",
    "\n",
    "    data.append(d)\n",
    "    for j in range(k):\n",
    "\n",
    "        d = {\"id\": \"g{n}\".format(n=j+1),\n",
    "            \"valueAxis\": \" \",\n",
    "            \"lineColor\": color[j],\n",
    "            \"bullet\": \"round\",\n",
    "            \"bulletBorderThickness\": 1,\n",
    "            \"hideBulletsCount\": 30,\n",
    "            \"title\": \"Topic{n}\".format(n=j+1),\n",
    "            \"valueField\": \"{n}\".format(n=j+1),\n",
    "            \"fillAlphas\": 0}\n",
    "\n",
    "        data.append(d)\n",
    "        \n",
    "    graph_data_json = json.dumps(data) #parsing    \n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(\"graph_data[{n}] = \".format(n=k) + repr(ast.literal_eval(graph_data_json)) + \"; \\n\")\n",
    "        f.close()  \n",
    "    t2 = time()\n",
    "    print(t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
