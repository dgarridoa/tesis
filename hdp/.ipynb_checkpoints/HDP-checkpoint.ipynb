{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "#Preprocesamiento\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS #importar set de stopwords\n",
    "from nltk.stem import SnowballStemmer #importar stemmer\n",
    "nlp = spacy.load('es_core_news_sm') #python -m spacy download es\n",
    "\n",
    "#Topic modeling\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "#Elimina los warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc, sep=None, vocabulary = None, homol_dict=None, lemmatization=False, stemming = False):\n",
    "    '''\n",
    "    Por defecto divide la sentencia por el carácter espacio.\n",
    "    Ej: 'Data Mining is the best course'->['Data',  'Mining', 'is', 'the', 'best', 'course']\n",
    "    \n",
    "    Input: \n",
    "    1. doc: str, documento.\n",
    "    2. sep: str, carácter para dividir el documento en tokens, por defecto es el espacio.\n",
    "    3. vocabuary: set, si un vocabulario es dado filtra las palabras que no estan presentes en el.\n",
    "    4. homol_dict: dict, diccionario de homologaciones.\n",
    "    5. lemmatization: bool, si es True lleva las palabras a su lema.\n",
    "    6. stemming: bool, si es True lleva las palabas a su raíz.\n",
    "    \n",
    "    Output: \n",
    "    list, lista de tokens.\n",
    "    \n",
    "    Nota: aplicar stemming y lemmatization al mismo tiempo no es correcto.\n",
    "    '''\n",
    "    doc = re.sub(r'\\S+@\\S+', '', doc) #elimina correos electrónicos  \n",
    "    doc = re.sub(r'[\\xa0]', '', doc) #elimina el patrón \\xa0\n",
    "    doc = re.sub(r'[^\\w\\s]','', doc) #elimina los símbolos de puntuación excepto underscore\n",
    "    doc = re.sub(r'[_]', '', doc) #elimina underscore\n",
    "    doc = re.sub(r'[a-zA-Z]+[0-9]+', '', doc) #elimina los tokens que contienen letras y números\n",
    "    doc = re.sub(r'([ø ÿ þ])', ' ', doc) #reemplazas los símbolos contenidos por un espacio \n",
    "    doc = re.sub(r'[0-9]', '', doc) #elimina los tokens númericos\n",
    "    tokens = doc.split(sep) #tokenización\n",
    "    tokens = [word.lower() for word in tokens] #pasar todas las palabras a minúsculas\n",
    "    \n",
    "    \n",
    "    if vocabulary is not None:\n",
    "        #solo considera caracteres que tokens que estan en vocabulary\n",
    "        tokens = [word for word in tokens if word in vocabulary]\n",
    "    \n",
    "    #if homol_dict is not None:\n",
    "    \n",
    "    if lemmatization==True:\n",
    "        tokens = [nlp(word)[0].lemma_ for word in tokens]\n",
    "        \n",
    "    if stemming == True:\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar la base de datos\n",
    "df = pd.read_pickle('../data/robos_prose_v1.pkl')\n",
    "df.head()\n",
    "\n",
    "#Importar vocabulario\n",
    "with open('../data/vocabulary.pickle', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus tokenizado utilizando el vocabulario\n",
    "data_tokenized = [tokenizer(doc, vocabulary=vocabulary) for doc in df['sin_relato']]\n",
    "\n",
    "#Creamos el diccionario a partir de los textos procesados en el formato que necesita LDA en gensim\n",
    "dictionary = Dictionary(data_tokenized)\n",
    "\n",
    "#Transformamos el corpus al formato que requiere la librería\n",
    "#El corpus contiene una representacion numerica de los textos, un texto es representado por una lista de tuplas\n",
    "#donde el primer elemento de la tupla es la id de la palabra y el segundo es su frecuencia de aparición en el texto.\n",
    "corpus = [dictionary.doc2bow(text) for text in data_tokenized]\n",
    "\n",
    "#Guardamos el diccionario y el corpus\n",
    "dictionary.save('../data/dictionary.dict')\n",
    "MmCorpus.serialize('../data/corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, alpha='auto', eta='auto', random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el modelo\n",
    "lda_model.save('lda_model.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mecanismo', 0.06720624),\n",
       " ('estacionado', 0.04111982),\n",
       " ('estacionamiento', 0.038681664),\n",
       " ('seguridad', 0.03460389),\n",
       " ('informados', 0.014400032),\n",
       " ('mall', 0.0138035305),\n",
       " ('casa', 0.012337404),\n",
       " ('encontraba', 0.010977054),\n",
       " ('vidrios', 0.01001468),\n",
       " ('ingresada', 0.008947119),\n",
       " ('ruido', 0.008045356),\n",
       " ('suelo', 0.007937963),\n",
       " ('ventana', 0.007869375),\n",
       " ('llaves', 0.007554966),\n",
       " ('vidrio', 0.00734365)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printear las topn palabras más probables de un tópico\n",
    "lda_model.show_topic(topicid=1, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary, sort_topics=True, R=30)\n",
    "pyLDAvis.display(lda_vis, template_type='notebook')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
